\section{Algorithms}

\subsection{Finding frequent episodes: high-level algorithm}

\begin{algorithm}

\caption{High-level algorithm for finding frequent episodes. \\
Input: A set $ \Sigma $ of event types, an event sequence $ s $ over $ \Sigma $, a window width \emph{win}, and a frequency threshold \emph{min\_fr}. \\
Output: The collection of episodes that are frequent in the sequence in terms of the input parameters.
}

\begin{algorithmic}[1]

\State $ \mathcal{C}_1 \gets \Sigma $
\State $ l \gets 1 $
\While{$ \mathcal{C}_l \neq \emptyset $}
    \LineComment{Database pass (algorithms \ref{alg:rec-par-fwi}, \ref{alg:rec-ser-fwi}, ...)}
    \State compute $ \mathcal{F}_l \gets \{ \alpha \in \mathcal{C}_l \mid fr(\alpha, s, \text{win}) \geq \text{min\_fr} \} $
    \State $ l \gets l + 1 $
    \LineComment{Candidate generation (algorithm \ref{alg:cand-gen})}
    \State compute $ C_l = \{ \alpha \mid | \alpha | = l \wedge \forall \beta \mid \beta \subset \alpha : \beta \text{ is frequent} \} $
\EndWhile
\State output all $ \mathcal{F}_i $

\end{algorithmic}

\label{alg:episodes-top-level}
\end{algorithm}

% TODO Read this future me, probably needs improvement.

Algorithm~\ref{alg:episodes-top-level} describes the high-level procedure to find all frequent episodes of a certain class (parallel or serial) that are frequent in an event sequence $ s $, given window width \emph{win} and frequency threshold \emph{min\_fr}. It is a breadth-first algorithm, that is, all frequent episodes of size $ l $ are computed before those of size $ l + 1 $.

The construction of the first set of candidates is a special case. $ \mathcal{C}_1 $ consists of simply all possible episodes with one node. That's one episode per event type, and so $ | C_1 | = | \Sigma | $. After constructing $ \mathcal{C}_1 $ a loop is entered where
ever-bigger episodes are constructed and filtered by frequency in the sequence.
% for each $ l $, it is determined which episodes are frequent ($ \mathcal{F}_1 $), by a pass over the sequence, and subsequently, based on those, candidates of size $ 2 $ are generated. The loop continues with ever-increasing episode size $ l $, and stops when no more candidates were generated. Then all frequent episodes have been found.

The following subsections will cover all of the subalgorithms.

\subsection{Candidate generation}
\label{sec:cand-gen}

\begin{algorithm}

\caption{Generating candidate parallel episodes of size $ l + 1 $ from frequent parallel episodes of size $ l $. \\
Input: A sorted array $ \mathcal{F}_l $ of frequent parallel episodes of size $ l $. \\
Output: A sorted array of candidate parallel episodes of size $ l + 1 $.
}

\begin{algorithmic}[1]

\State $ \mathcal{C}_{l + 1} \gets \emptyset $
\State $ k \gets 0 $
\If{$ l = 1 $}
    \For{$ h \gets 1 $ to $ | \mathcal{F}_l | $} $ \mathcal{F}_l \text{.block\_start}[h] \gets 1 $ \EndFor
\EndIf
\For{$ i \gets 1 $ to $ | \mathcal{F}_l | $} \label{alglin:cand-gen:i-loop}
    \State $ \text{current\_block\_start} \gets k + 1 $
    \For{$ j \gets i $; $ \mathcal{F}_l \text{.block\_start}[j] = \mathcal{F}_l \text{.block\_start}[i] $; $ j \gets j + 1 $}
    \label{alglin:cand-gen:j-loop}
        \LineComment{$ \mathcal{F}_l[i] $ and $ \mathcal{F}_l[j] $ have $ l - 1 $ first event types in common, build a potential candidate $ \alpha $ as their combination.}
        \For{$ x \gets 1 $ to $ l $} $ \alpha[x] \gets \mathcal{F}_l[i][x] $ \label{alglin:cand-gen:construct-candidate-1}
        \EndFor
        \State $ \alpha[l + 1] \gets \mathcal{F}_l[j][l] $ \label{alglin:cand-gen:construct-candidate-2}
        \LineComment{Build and test subepisodes $ \beta $ that do not contain $ \alpha[y] $}
        \For{$ y \gets 1 $ to $ l - 1 $} \label{alglin:cand-gen:test-subepisodes-loop}
            \For{$ x \gets 1 $ to $ y - 1 $} $ \beta[x] \gets \alpha[y] $
            \EndFor
            \For{$ x \gets y $ to $ l $} $ \beta[x] \gets \alpha[x + 1] $
            \EndFor
            \If{$ \beta $ is not in $ \mathcal{F}_l} $ continue with the next $ j $ at line~\ref{alglin:cand-gen:j-loop}
            \EndIf
        \EndFor
        \LineComment{All subepisodes are in $ \mathcal{F}_l $, store $ \alpha $ as candidate}
        \State $ k \gets k + 1 $
        \State $ \mathcal{C}_{l + 1}[k] \gets \alpha $
        \State $ \mathcal{C}_{l + 1} \text{.block\_start}[k] \gets \text{current\_block\_start} $
    \EndFor
\EndFor
\State output $ \mathcal{C}_{l + 1} $

\end{algorithmic}

\label{alg:cand-gen}
\end{algorithm}

% TODO show modification for serial episodes

Algorithm~\ref{alg:cand-gen} generates candidates of size $ l + 1 $ from a collection of frequent parallel episodes of size $ l $. It can be easily modified to generate serial episodes, as we'll show later in this subsection. Thanks to the monotonic property of the frequency measures we implement, some candidates can be immediately proven infrequent. More specifically, if there exists an infrequent subepisode $ \beta $ of candidate $ \alpha $, then $ \alpha $ is not frequent.
Conveniently, all frequent subepisodes of size $ l $ are already given as input to the algorithm. So when the algorithm constructs a candidate of size $ l + 1 $, all of its subepisodes of size $ l $ are checked to be frequent by testing whether they are in the input collection $ \mathcal{F}_l $. If one of the subepisodes is not in $ \mathcal{F}_l $, then it is infrequent and, consequently, the potential candidate cannot be frequent either.
Subepisodes smaller than $ l $ do not need to be checked anymore, because any less-than-$ l $-sized subepisode of the potential candidate is also a subepisode of one of the $ l $-sized subepisodes and any $ l $-sized subepisodes that have turned out to be frequent are already known to have frequent subepisodes.

% Combine this paragraph with the one about parallel episode candidate generation?
Parallel episodes are constructed such that the elements in their arrays are sorted according to event type. In this way, a parallel episode has a unique array representation.

Potential candidates of size $ l + 1 $ are generated by combining frequent episodes of size $ l $ which share a prefix of $ l - 1 $ elements. In other words, they are only allowed to differ in their last element. The newly constructed candidate shares these $ l - 1 $ as well, and is then followed by the last elements of both $ \alpha $ and $ \beta $. This method of candidate generation is similar to the way in which candidate itemsets are generated in the Apriori \cite{apriori97} algorithm. Lines~\ref{alglin:cand-gen:construct-candidate-1} and \ref{alglin:cand-gen:construct-candidate-2} implement this. Figure~\ref{fig:parallel-episode-combined} shows an example of two episodes being combined, and figure~\ref{fig:parallel-episode-lattice} shows visually how larger candidates build upon smaller episodes.

\begin{figure}
\centering

\begin{tikzpicture}

\newcommand\sharedprefix[1]
{
    \ifcase#1 A
    \or A
    \or B
    \fi
}

\foreach \arrayindex [evaluate=\arrayindex as \leftx using int(\arrayindex-4),
                      evaluate=\arrayindex as \rightx using (int(\arrayindex+1))] in {0,...,2}
{
    \node (n\arrayindex0) [arraycell] at (\leftx,0.75) {$ \sharedprefix{\arrayindex} $};
    \node (n\arrayindex1) [arraycell] at (\leftx,-0.75) {$ \sharedprefix{\arrayindex} $};
    \node (n\arrayindex2) [arraycell] at (\rightx,0) {$ \sharedprefix{\arrayindex} $};
}

\node (n30) [arraycell,color=red!90!black] at (-1,0.75) {$ B $};
\node (n31) [arraycell,color=blue!90!black] at (-1,-0.75) {$ C $};

\node (n32) [arraycell,color=red!90!black] at (4,0) {$ B $};
\node (n42) [arraycell,color=blue!90!black] at (5,0) {$ C $};

\draw [->] (n30.north) to [bend left=45] (n32.north);
\draw [->] (n31.south) to [bend right=45] (n42.south);

\draw [accolade] (n21.south east) -- node (acc1tip) [inner sep=0,midway,yshift=-10pt] {} (n01.south west);
\draw [accolade] (n22.south east) -- node (acc2tip) [inner sep=0,midway,below,yshift=-10pt] {} (n02.south west);

\draw [->] (acc1tip) to [bend right=45] (acc2tip);

% \node [above=of n00] {first episode};
% \node [below=of n01] {second episode};

\node [right=5pt of n42] {potential candidate};

\end{tikzpicture}

\caption{Two episodes being combined into a larger episode.}
\label{fig:parallel-episode-combined}
\end{figure}

\begin{figure}
\centering

\begin{tikzpicture}

\node (e) at (0,0) {$ \emptyset $};
\node (A) at (-1,-1) {$ A $};
\node (B) at (0,-1) {$ B $};
\node (C) at (1, -1) {$ C $};

\node (AA) at (-2.5,-2) {$ AA $};
\node (AB) at (-1.5,-2) {$ AB $};
\node (AC) at (-0.5,-2) {$ AC $};
\node (BB) at (0.5,-2) {$ BB $};
\node (BC) at (1.5,-2) {$ BC $};
\node (CC) at (2.5,-2) {$ CC $};

\node (AAA) at (-6,-3.5) {$ AAA $};
\node (AAB) at (-4.5,-3.5) {$ AAB $};
\node (AAC) at (-3,-3.5) {$ AAC $};
\node (ABB) at (-1.5,-3.5) {$ ABB $};
\node (ABC) at (0,-3.5) {$ ABC $};
\node (BBB) at (1.5,-3.5) {$ BBB $};
\node (BBC) at (3,-3.5) {$ BBC $};
\node (BCC) at (4.5,-3.5) {$ BCC $};
\node (CCC) at (6,-3.5) {$ CCC $};

\draw (e) -- (A);
\draw (e) -- (B);
\draw (e) -- (C);

\draw (A) -- (AA);
\draw (A) -- (AB);
\draw (B) -- (AB);
\draw (A) -- (AC);
\draw (C) -- (AC);
\draw (B) -- (BB);
\draw (B) -- (BC);
\draw (C) -- (BC);
\draw (C) -- (CC);

\draw (AA) -- (AAA);
\draw (AA) -- (AAB);
\draw (AB) -- (AAB);
\draw (AB) -- (ABB);
\draw (AA) -- (AAC);
\draw (AC) -- (AAC);
\draw (AB) -- (ABC);
\draw (AC) -- (ABC);
\draw (BB) -- (BBB);
\draw (BB) -- (BBC);
\draw (BC) -- (BBC);
\draw (BC) -- (BCC);
\draw (CC) -- (BCC);
\draw (CC) -- (CCC);

\end{tikzpicture}

\caption{Parallel episode construction for $ \Sigma = \{ A, B, C \} $ up to size 3.}

\label{fig:parallel-episode-lattice}
\end{figure}

The algorithm assumes that the input array of frequent $ l $-sized episodes $ \mathcal{F}_l $ is sorted lexicographically. That is, in the first place the episodes are sorted by the first event type in the array, secondarily by the second element, and so on. The algorithm constructs new episodes in such a way that the output is also ordered lexicographically, and since the ordering is preserved when filtering episodes, the assumption about the input is always true.

As mentioned previously, a potential candidate is generated from two episodes that share an $ (l - 1) $-prefix in their array representation. And with the episodes being ordered as described, all of the episodes that share a prefix are grouped together. As a result, episodes can be grouped into \emph{blocks}, where all of the episodes in a block share the first $ l - 1 $ elements. Then all episodes within the same block are combined. Figure~\ref{fig:blocks} illustrates the block structure.

\newcommand\blockspicvalue[2]{
    \ifcase#2
        \ifcase#1 A
            \or A
            \or B
        \fi
        \or \ifcase#1 A
            \or A
            \or C
        \fi
        \or \ifcase#1 A
            \or C
            \or C
        \fi
        \or \ifcase#1 A
            \or C
            \or D
        \fi
        \or B
        \or \ifcase#1 B
            \or B
            \or C
        \fi
        \or \ifcase#1 B
            \or B
            \or D
        \fi
        \or C
    \fi
}

\begin{figure}
\centering

\begin{tikzpicture}

\foreach \x in {0,...,2}
\foreach \y in {0,...,7}
{
    \node (n\x\y) [draw,minimum size=1cm] at (\x,-\y*1.1) {$ \blockspicvalue{\x}{\y} $};
}

\draw[accolade]([yshift=2pt]n00.north west) -- ([yshift=2pt]n10.north east) node [midway,above=12pt] {episodes with shared 2-prefix form a block};

\draw [accolade] ([xshift=2pt]n20.north east) -- ([xshift=2pt]n21.south east) node [midway,right=12pt] {block};
\draw [accolade] ([xshift=2pt]n22.north east) -- ([xshift=2pt]n23.south east) node [midway,right=12pt] {block};
\draw [accolade] ([xshift=2pt]n24.north east) -- ([xshift=2pt]n26.south east) node [midway,right=12pt] {block};
\draw [decorate,decoration={brace,amplitude=5pt}] ([xshift=2pt]n27.north east) -- ([xshift=2pt]n27.south east) node [midway,right=12pt] {block};

\end{tikzpicture}

\caption{Blocks in candidate generation algorithm with parallel episodes of size 3.}

\label{fig:blocks}
\end{figure}

The block structure is represented as follows.
Each episode $ \alpha $ is associated with a value \emph{block\_start}, which is the array index of the first episode in the block which contains $ \alpha $, so it points ``back'' to the beginning of the block. The first episode in each block points to itself. In this way, it can be easily tested whether two episodes belong to the same block. Figure~\ref{fig:block-values} shows this representation for the episodes in figure~\ref{fig:blocks}.

\begin{figure}
\centering

\begin{tikzpicture}

\newcommand\blockspicvaluevalue[1]
{
    \ifcase#1 1
    \or 1
    \or 3
    \or 3
    \fi
}

\foreach \y [evaluate=\y as \arrayindex using int(\y+1)] in {0,...,3}
{
    \foreach \x in {0,...,2}
    {
        \node (n\x\y) [draw,minimum size=1cm] at (\x,-\y*1.1) {$ \blockspicvalue{\x}{\y} $};
    }
    \node [left=10pt of n0\y] {$ \arrayindex $};
    \node [right=10pt of n2\y] {$ \blockspicvaluevalue{\y} $};
}

\node [above left=0 of n00] {array index};
\node [above right=0 of n20] {block\_value};
\node [below=0 of n13] {$ \vdots $};

\end{tikzpicture}

\caption{How blocks are represented in the candidate generation algorithm.}
\label{fig:block-values}
\end{figure}

The algorithm constructs this block structure while generating candidates of size $ l + 1 $, but it also uses the blocks of the $ l $-sized episodes given as input. So it is important that they are preserved and maintained in between different runs of algorithm~\ref{alg:cand-gen}. Section~\ref{sec:maintain-blocks} goes into more detail about this.

We mentioned earlier that in the array representation of parallel episodes, elements are sorted according to event type. We would like to construct parallel candidates for which this is the case as well. We know that the input array of frequent episodes is sorted. Consequently, for any two episodes $ \mathcal{C}[i] $ and $ \mathcal{C}[j] $ in a block such that $ i \leq j $, it holds that $ \mathcal{C}[i][l] \leq \mathcal{C}[j][l] $.
Now, if we always construct a new candidate $ \alpha $ as $ \langle \mathcal{C}[i][1], \ldots,\allowbreak\mathcal{C}[i][l],\allowbreak\mathcal{C}[j][l] \rangle $ and choose $ i $ and $ j $ such that $ i \leq j $, then $ \alpha $'s array representation is sorted by construction. This explains the nested-loop structure of the algorithm (lines~\ref{alglin:cand-gen:i-loop} and~\ref{alglin:cand-gen:j-loop}), where the inner array index $ j $ is always greater than or equal to the outer index $ i $.

With serial episodes on the other hand, the order of elements in the array is not bound by an order on the event types.
Serial candidate generation can be accomplished by a small change to the algorithm. Line~\ref{alglin:cand-gen:j-loop} needs to be changed to:

\begin{algorithmic}[0]
\For{$ j \gets \mathcal{F}_l. \text{block\_start}[i] $; $ \mathcal{F}_l \text{.block\_start}[j] = \mathcal{F}_l \text{.block\_start}[i] $; $ j \gets j + 1 $}
\EndFor
\end{algorithmic}

After constructing a potential candidate (lines~\ref{alglin:cand-gen:construct-candidate-1} and \ref{alglin:cand-gen:construct-candidate-2}), it needs to be checked whether all of its subepisodes are frequent.
Constructing all $ l $-sized subepisodes of an $ (l + 1) $-sized candidate $ \alpha $ is straightforward. For each subepisode, one node from $ \alpha $'s graph is left out, until all nodes have been left out. For serial episodes, the edges of the subepisode are constructed such that the order of the toplogical sort is preserved. If we consider the strict form of a serial candidate, an $ l $-sized subepisode can be obtained by leaving one node $ v $ and all edges that contain $ v $. For the array representation of both classes of episodes, this means that one element of the array is left out. See figure~\ref{fig:cand-subepisodes} for an example. In the algorithm, this happens at lines~\ref{alglin:cand-gen:test-subepisodes-loop} and further. The variable $ y $ denotes the array index of the array element that will be left out.

For any potential candidate, two of subepisodes in particular are already known to be frequent, namely the episodes it was built from. Those subepisodes are obtained by leaving out the last and the second to last array elements. That's why the largest value $ y $ takes in the algorithm is $ l - 1 $.

We should note that in the subepisode construction procedure described above, some of the subepisodes may be equivalent to each other, when an event type occurs multiple times in a parallel episode. For $ \{ A, A, B \} $, subepisode $ \{ A, B \} $ will be constructed twice.

\begin{figure}
\centering

\begin{tikzpicture}

\node at (2,1) {potential 4-candidate};
\node at (6,2.65) {3-subepisodes};

\node [arraycell] at (0.5,0) {$ A $};
\node [arraycell] at (1.5,0) {$ B $};
\node [arraycell] at (2.5,0) {$ C $};
\node [arraycell] at (3.5,0) {$ D $};

\node [arraycell] at (5,1.65) {$ B $};
\node [arraycell] at (6,1.65) {$ C $};
\node (r0) [arraycell] at (7,1.65) {$ D $};
\node [right=10pt of r0] {drop $ A $};
\draw [red,very thick] (4.5,2.15) -- ++(0,-1);

\node [arraycell] at (5,0.55) {$ A $};
\node [arraycell] at (6,0.55) {$ C $};
\node (r1) [arraycell] at (7,0.55) {$ D $};
\node [right=10pt of r1] {drop $ B $};
\draw [red,very thick] (5.5,1.05) -- ++(0,-1);

\node [arraycell] at (5,-0.55) {$ A $};
\node [arraycell] at (6,-0.55) {$ B $};
\node (r2) [arraycell] at (7,-0.55) {$ D $};
\node (dropC) [right=10pt of r2] {drop $ C $};
\draw [red,very thick] (6.5,-0.05) -- ++(0,-1);

\node [arraycell] at (5,-1.65) {$ A $};
\node [arraycell] at (6,-1.65) {$ B $};
\node (r3) [arraycell] at (7,-1.65) {$ C $};
\node (dropD) [right=10pt of r3] {drop $ D $};
\draw [red,very thick] (7.5,-1.15) -- ++(0,-1);

\draw [accolade] (dropC.east |- r2.north) -- node [midway,right,align=left,xshift=10pt] {last two are frequent\\since candidate was\\built upon them} (dropD.east |- r3.south);

\end{tikzpicture}

\caption{The construction of $ l $-sized subepisodes for an $ (l + 1) $-sized candidate.}
\label{fig:cand-subepisodes}
\end{figure}



\subsection{Recognizing parallel episodes with fixed windows}

\begin{algorithm}

\caption{Recognizing parallel episodes using the fixed-window frequency measure. \\
Input: A collection $ \mathcal{C} $ of parallel episodes, an event sequence $ \boldsymbol{s} = (s, T_s, T_e) $, a window width \textit{win}, and a frequency threshold \textit{min\_fr}. \\
Output: The episodes of $ \mathcal{C} $ that are frequent in $ \boldsymbol{s} $ with respect to \textit{win} and \textit{min\_fr}.
}

\begin{algorithmic}[1]

\LineComment{Initialization}
\ForAll{$ \alpha $ in $ \mathcal{C} $}
    \ForAll{$ A $ in $ \alpha $}
        \State $ A \text{.count} \gets 0 $
        \For{$ i \gets 1 $ to $ | \alpha | $} $ \text{contains}(A, i) \gets \emptyset $ \EndFor
    \EndFor
\EndFor

\ForAll{$ \alpha $ in $ \mathcal{C} $}
    \ForAll{$ A $ in $ \alpha $}
        \State $ a \gets $ number of elements of type $ A $ in $ \alpha $
        \State $ \text{contains}(A, a) \gets \text{contains}(A, a) \cup \{ \alpha \} $
    \EndFor
    \State $ \alpha \text{.event\_count} \gets 0 $
    \State $ \alpha \text{.freq\_count} \gets 0 $
\EndFor

\LineComment{Recognition}
\For{$ \text{start} \gets T_s - \text{win} + 1 $ to $ T_e $}
    \LineComment{Bring new events to the window}
    \ForAll{events $ (A, t) $ in $ s $ such that $ t = \text{start} + \text{win} - 1 $} \label{alglin:rec-par-fwi:new-events}
        \State $ A \text{.event\_count} \gets A \text{.event\_count} + 1 $
        \ForAll{$ \alpha \in \text{contains}(A, A \text{.count}) $}
            \State $ \alpha \text{.event\_count} \gets \alpha \text{.event\_count} + A \text{.count} $
            \If{$ \alpha \text{.event\_count} = | \alpha | $} $ \alpha \text{.in\_window} \gets \text{start} $
            \EndIf
        \EndFor
    \EndFor
    \LineComment{Drop out old events from the window}
    \ForAll{events $ (A, t) $ in $ s $ such that $ t = \text{start} - 1 $} \label{alglin:rec-par-fwi:old-events}
        \ForAll{$ \alpha \in \text{contains}(A, A \text{.count}) $}
            \If{$ \alpha \text{.event\_count} = | \alpha | $}
                \State $ \alpha \text{.freq\_count} \gets \alpha \text{.freq\_count} - \alpha \text{.in\_window} + \text{start} $
            \EndIf
            \State $ \alpha \text{.event\_count} \gets \alpha \text{.event\_count} - A \text{.count} $
        \EndFor
        \State $ A \text{.event\_count} \gets A \text{.event\_count} - 1 $
    \EndFor
\EndFor
\LineComment{Output}
\ForAll{episodes $ \alpha $ in $ \mathcal{C} $}
    \If{$ \alpha \text{.freq\_count} / (T_e - T_s + \text{win} - 1) \geq \text{min\_fr} $} output $ \alpha $
    \EndIf
\EndFor

\end{algorithmic}

\label{alg:rec-par-fwi}
\end{algorithm}

Algorithm~\ref{alg:rec-par-fwi} recognizes parallel episodes in an event sequence. As stated before, parallel episodes impose no order on the occurrence of events in the window. Therefore, to recognize an episode in the sequence, it suffices to know that there are enough events of each type currently in the window. More formally, a parallel episode $ \alpha $ occurs in a window if for each event type $ A $, the window contains at least as many events of type $ A $ as there are nodes of type $ A $ in $ \alpha $'s graph.

The algorithm makes one pass over the sequence, timestamp by timestamp, using a sliding window of size \emph{win}. At any point during iteration, there are a few timestamps of interest.

In the algorithm---and further algorithms that iterate the sequence---variable \emph{start} always refers to the smallest timestamp of the current window. It is the timestamp that will be dropped from the window the following iteration; at that timestamp we find the ``oldest'' events still in the window. The ``newest'' events in the window---the events which just entered the window---are found at $ (\text{start} + \text{win} - 1) $. We'll call the former the \emph{back} of the sliding window, and the latter the \emph{front}.

In order to prevent having to consider a special case for the beginning and the end of the iteration over the sequence, the algorithm starts with just the first valid timestamp $ T_s $ at the front of the sliding window. All other timestamps within the window are outside of the sequence at this point. In terms of subwindows, this first window can be written as $ s[T_s - \text{win} + 1, T_s + 1] $. Strictly speaking, this isn't a valid subwindow, since its range lies outside the range of the sequence itself, but we can imagine extending the sequence on both sides to allow for this. At the end of the sequence, the same happens; the last subwindow of the iteration is $ s[T_e, T_e + \text{win}] $. Figure~\ref{fig:sliding-window} shows this way of iterating visually.

\begin{figure}
\centering

\begin{tikzpicture}

\def\interdotdistance{0.8}
\def\slidingwindowheight{0.5}

\newcommand\slidingwindowthingy[3]
{
    \foreach \i [evaluate=\i as \x using \i * \interdotdistance] in {0,...,6}
    {
        \fill (\x,#1) circle [color=black,radius=3pt] node (n#1\i) {};
    }
    \draw ({(-0.5+#2)*\interdotdistance},#1-0.5*\slidingwindowheight) rectangle +(3*\interdotdistance,\slidingwindowheight);
    \ifcase #3
        \node [right=10pt of n#16] {$ \cdots $};
    \or
        \node [left=10pt of n#10] {$ \cdots $};
    \fi
}

\slidingwindowthingy{0}{-2}{0}
\slidingwindowthingy{-1}{-1}{0}
\slidingwindowthingy{-2}{0}{0}

\node at (2.4,-2.9) {$ \vdots $};

\slidingwindowthingy{-4}{4}{1}
\slidingwindowthingy{-5}{5}{1}
\slidingwindowthingy{-6}{6}{1}

\node [above=10pt of n00] {$ T_s $};
\node [below=10pt of n-66] {$ T_e $};

\end{tikzpicture}
\caption{Visualization of the way algorithm~\ref{alg:rec-par-fwi} and further algorithms pass a sliding window over the sequence. Black dots represent timestamps that belong to the sequence, regardless of whether events occur at that timestamp.}
\label{fig:sliding-window}
\end{figure}

Recognition is accomplished as follows. For each event type $ A $, a counter $ A \text{.count} $ is maintained. This counter denotes how many events of type $ A $ are currently in the window. When a new event of type $ A $ enters the window (line~\ref{alglin:rec-par-fwi:new-events} and further), this counter is increased by $ 1 $. Then, for all episodes $ \alpha $ containing $ A \text{.count} $ nodes of type $ A $, a per-episode counter $ \alpha \text{.event\_count} $ is increased by $ A \text{.count} $, indicating that there are currently enough events of type $ A $ in the window to satisfy an occurrence. If this is the case for all event types in $ \alpha $, then $ \alpha \text{.event\_count} = | \alpha | $, and so the window contains $ \alpha $. To mark the point in time at which the episode entered the sliding window, variable $ \alpha \text{.in\_window} $ gets set to \emph{start}.

When an event of type $ A $ leaves the window (line~\ref{alglin:rec-par-fwi:old-events} and further), for all episodes $ \alpha $ with $ A \text{.count} $ nodes of event type $ A $, if $ \alpha \text{.event\_count} = | \alpha | $, $ \alpha $ has been contained in a number of windows, but is now no longer. The episode was in $ (\text{start} - \alpha \text{.in\_window}) $ windows, and so $ \alpha \text{.freq\_count} $ gets updated accordingly. Next, $ A \text{.count} $ gets decreased by $ 1 $ to reflect the fact that an event of type $ A $ has left the sliding window.

Figure~\ref{fig:parallel-recognition} illustrates this.

\begin{figure}
\centering

\begin{tikzpicture}


\def\slidingwindowheight{0.5}
\def\interdotdistance{0.6}

\newcommand\letteratposition[1]
{\ifnum#1=1A\else\ifnum#1=3B\else\ifnum#1=6A\else\ifnum#1=7A\else0\fi\fi\fi\fi}

\newcommand\slidingwindowthingy[2]
{
    \foreach \i [evaluate=\i as \x using \i * \interdotdistance] in {0,...,8}
    {
        \ifnum\pdfstrcmp{\letteratposition{\i}}{0}=0
            \fill (\x,#1) circle [color=black,radius=2pt] node (n#2-\i) {};
        \else
            \draw (\x,#1) node (n#2-\i) {$ \letteratposition{\i} $};
        \fi
    }

    \node (dr#2) [right=10pt of n#2-8] {$ \cdots $};
    \node (dl#2) [left=10pt of n#2-0] {$ \cdots $};
}

\slidingwindowthingy{0}{0}
\draw (-0.5*\interdotdistance,-0.5*\slidingwindowheight) -- ++(2*\interdotdistance,0) -- ++(0,\slidingwindowheight) -- ++(-2*\interdotdistance,0);
\node (Acount) [right=1em of dr0,rotate=90,anchor=west,xshift=0.6cm] {$ A \text{.count} $};
\node (Bcount) [right=2.5em of dr0,rotate=90,anchor=west,xshift=0.6cm] {$ B \text{.count} $};
\node (alphaeventcount) [right=4em of dr0,rotate=90,anchor=west,xshift=0.6cm] {$ \alpha \text{.event\_count} $};

\node at (Acount |- dr0) {1};
\node at (Bcount |- dr0) {0};
\node at (alphaeventcount |- dr0) {0};

\slidingwindowthingy{-1}{1}
\draw (-0.5*\interdotdistance,-1-0.5*\slidingwindowheight) -- ++(4*\interdotdistance,0) -- ++(0,\slidingwindowheight) -- ++(-4*\interdotdistance,0);
\node at (Acount |- dr1) {1};
\node at (Bcount |- dr1) {1};
\node at (alphaeventcount |- dr1) {1};

\slidingwindowthingy{-3}{2}
\draw (-0.5*\interdotdistance,-3-0.5*\slidingwindowheight) rectangle ++(7*\interdotdistance,\slidingwindowheight);
\node at (Acount |- dr2) {2};
\node at (Bcount |- dr2) {1};
\node at (alphaeventcount |- dr2) {3};
\node [right=5.5em of dr2,align=left] {$ \alpha \text{.event\_count} = | \alpha | $\\$ \Rightarrow \alpha $ recognized};
\node (inwindowtxt) [above=0.4cm of n2-0,align=center] {save location of sliding\\window (\emph{in\_window})};
\draw [->] (inwindowtxt) -- (n2-0);

\slidingwindowthingy{-4}{3}
\draw (0.5*\interdotdistance,-4-0.5*\slidingwindowheight) rectangle ++(7*\interdotdistance,\slidingwindowheight);
\node at (Acount |- dr3) {3};
\node at (Bcount |- dr3) {1};
\node at (alphaeventcount |- dr3) {3};

\slidingwindowthingy{-5}{4}
\draw (1.5*\interdotdistance,-5-0.5*\slidingwindowheight) rectangle ++(7*\interdotdistance,\slidingwindowheight);
\node at (Acount |- dr4) {2};
\node at (Bcount |- dr4) {1};
\node at (alphaeventcount |- dr4) {3};

\slidingwindowthingy{-6}{5};
\draw (8.5*\interdotdistance,-6-0.5*\slidingwindowheight) -- ++(-5*\interdotdistance,0) -- ++(0,\slidingwindowheight) -- ++(5*\interdotdistance,0);
\node at (Acount |- dr5) {2};
\node at (Bcount |- dr5) {0};
\node at (alphaeventcount |- dr5) {2};
\node [right=5.5em of dr5,align=left] {$ \alpha \text{.event\_count} < | \alpha | $\\$ \Rightarrow \alpha $ not in window,\\determine no. windows};

\draw (n5-0.south) -- ++(0,-6pt) -- node (numwindowsindicator) [midway,below,align=center] {displacement of window\\since recognition} ++(4*\interdotdistance,0) -- ++(n5-4);

\end{tikzpicture}

\caption{Recognition of parallel episode $ \alpha = \{ A, A, B \} $ using the fixed window frequency measure. Black dots are timestamps which don't contain $ A $ or $ B $.}
\label{fig:parallel-recognition}
\end{figure}

\subsection{Recognizing serial episodes with fixed windows}

\begin{algorithm}

\caption{Recognizing serial episodes using the fixed-window frequency measure. \\
Input: A collection $ \mathcal{C} $ of serial episodes, an event sequence $ \boldsymbol{s} = (s, T_s, T_e) $, a window width \textit{win}, and a frequency threshold \textit{min\_fr}. \\
Ouptut: The episodes of $ \mathcal{C} $ that are frequent in $ \boldsymbol{s} $ with respect to \textit{win} and \textit{min\_fr}.
}

\begin{algorithmic}[1]

\LineComment{Initialization}
\ForAll{$ \alpha \in \mathcal{C} $}
    \For{$ i \leftarrow 1 $ to $ | \alpha | $}
        \State{$ \alpha \text{.initialized} \leftarrow \text{\textit{uninitialized}} $}
        \State{$ \text{waits}(\alpha[i]) \leftarrow \emptyset $}
    \EndFor
\EndFor

\ForAll{$ \alpha \in \mathcal{C} $}
    \State{$ \text{waits}(\alpha[1]) \leftarrow \text{waits}(\alpha[1]) \cup \left\{ \left( \alpha, 1 \right) \right\} $} \label{alglin:rec-ser-fwi:fill-waits-init}
    \State{$ \alpha \text{.freq\_count} \leftarrow 0 $}
\EndFor

\For{$ t \leftarrow T_s - \text{win} $ to $ T_s - 1 $} $ \text{begins\_at}(t) \leftarrow \emptyset $
\EndFor

\LineComment{Recognition}
\For{$ \text{start} \leftarrow T_s - \text{win} + 1 $ to $ T_e $} \label{alglin:rec-ser-fwi:iterate-sequence}
    \State{$ \text{begins\_at}(\text{start} + \text{win} - 1) \leftarrow \emptyset $}
    \State{$ \text{transitions} \leftarrow \emptyset $}
    \ForAll{events $ (A, t) $ in $ s $ such that $ t = \text{start} + \text{win} - 1 $} \label{alglin:rec-ser-fwi:iterate-new-events}
        \ForAll{$ ( \alpha, j) \in \text{waits}(A) $}
            \If{$ j = | \alpha | \wedge \alpha \text{.initialized}[j] = \text{\textit{uninitialized}} $} \label{alglin:rec-ser-fwi:mark-enter}
                \State{$ \alpha \text{.in\_window} \leftarrow \text{start} $}
            \EndIf
            \If{$ j = 1 $} \label{alglin:rec-ser-fwi:add-to-transitions}
                \State{$ \text{transitions} \leftarrow \text{transitions} \cup \{ ( \alpha, 1, \text{start} + \text{win} - 1 ) \} $}
            \Else
                \State{$ \text{transitions} \leftarrow \text{transitions} \cup \{ \alpha, j, \alpha \text{.initialized} [j - 1] \} $}
                \State{$ \text{begins\_at}( \alpha \text{.initialized}[j - 1] ) \leftarrow $ \label{alglin:rec-ser-fwi:cleanup-old-states}
                \State \hspace{\algorithmicindent} $ \text{begins\_at}( \alpha \text{.initialized}[j - 1] ) \setminus \{ ( \alpha, j - 1 ) \} $}
                \State{$ \alpha \text{.initialized} [j - 1] \leftarrow \text{\textit{uninitialized}} $}
                \State{$ \text{waits}(A) \leftarrow \text{waits}(A) \setminus \{ ( \alpha, j ) \} $}
            \EndIf
        \EndFor
    \EndFor
    \ForAll{$ ( \alpha, j, t ) \in \text{transitions} $}
        \State{$ \alpha \text{.initialized} [j] \leftarrow t $} \label{alglin:rec-ser-fwi:transition-begin}
        \State{$ \text{begins\_at}(t) \leftarrow \text{begins\_at}(t) \cup \{ ( \alpha, j ) \} $}
        \If{$ j < | \alpha | $}
            \State{$ \text{waits}(\alpha [j + 1]) \leftarrow \text{waits}(\alpha [j + 1]) \cup \{ (\alpha, j + 1) \} $} \label{alglin:rec-ser-fwi:transition-end}
        \EndIf
    \EndFor
    \ForAll{$ (\alpha, l) \in \text{begins\_at}(\text{start} - 1) $} \label{alglin:rec-ser-fwi:cleanup-for}
        \If{$ l = | \alpha | $} \label{alglin:rec-ser-fwi:cleanup-iteration-begin}
            \State{$ \alpha \text{.freq\_count} \leftarrow \alpha \text{.freq\_count} - \alpha \text{.in\_window} + \text{start} $}
        \Else
            \State{$ \text{waits}(\alpha [l + 1]) \leftarrow \text{waits}(\alpha [l + 1]) \setminus \{ ( \alpha, l + 1 ) \} $}
        \EndIf
        \State{$ \alpha \text{.initialized}[l] \leftarrow \text{\textit{uninitialized}} $} \label{alglin:rec-ser-fwi:cleanup-iteration-end}
    \EndFor
\EndFor
\ForAll{episodes $ \alpha $ in $ \mathcal{C} $}
    \If{$ \alpha \text{.freq\_count} / (T_e - T_s + \text{win} - 1) \geq \text{min\_fr} $}
        \State{output $ \alpha $}
    \EndIf
\EndFor

\end{algorithmic}

\label{alg:rec-ser-fwi}
\end{algorithm}

Algorithm~\ref{alg:rec-ser-fwi} is for recognizing serial episodes in a sequence. By definition serial episodes impose a total order on the nodes, and consequently on the events in the sequence in order to satisfy an occurrence of an episode. Serial episodes are therefore recognized using simple automata, instances of which advance state as events are encountered. Just like the algorithm for recognizing parallel episodes, algorithm~\ref{alg:rec-ser-fwi} iterates over the sequence once.

Each episode has its own automaton, which consists of $ | \alpha | $ states: each state corresponds to a node in the episode. For an episode $ A_1 \to A_2 \to \cdots A_n $, state $ j $ refers to the node with event type $ A_j $. Then an instance of the automaton for $ \alpha $ being in a state $ j $ denotes that the episode has been recognized up to (and including) the $ j $-th node. When in state $ j $ and upon encountering an event of which the type corresponds to the type of the $ (j + 1) $-th node of the episode, the instance will transition to state $ j + 1 $.

When an automaton instance of $ \alpha $ reaches state $ | \alpha | $, the episode has been successfully recognized, and from that point on, the episode will occur in a number of windows as the sliding window slides over the sequence.

Regardless of the state an automaton instance reached, the instance is removed when the timestamp at which the instance was initialized falls out of the sliding window.

Figure~\ref{fig:serial-recognition} illustrates the principle with an example.

\begin{figure}
\centering

\begin{tikzpicture}

\def\slidingwindowheight{0.5}
\def\interdotdistance{0.6}

\newcommand\letteratposition[1]
{\ifnum#1=2B\else\ifnum#1=5A\else\ifnum#1=7C\else0\fi\fi\fi}

\newcommand\slidingwindowthingy[2]
{
    \foreach \i [evaluate=\i as \x using \i * \interdotdistance] in {0,...,9}
    {
        \ifnum\pdfstrcmp{\letteratposition{\i}}{0}=0
            \fill (\x,#1) circle [color=black,radius=2pt] node (n#2-\i) {};
        \else
            \draw (\x,#1) node (n#2-\i) {$ \letteratposition{\i} $};
        \fi
    }

    \node (dr#2) [right=10pt of n#2-9] {$ \cdots $};
    \node (dl#2) [left=10pt of n#2-0] {$ \cdots $};
}

\slidingwindowthingy{0}{0}
\draw (-0.5*\interdotdistance,-0.5*\slidingwindowheight) -- ++(3*\interdotdistance,0) -- ++(0,\slidingwindowheight) -- ++(-3*\interdotdistance,0);
\node [right=10pt of dr0,align=left] {encountered $ B $;\\initialize new automaton,\\wait for A};
\node (initializedtxt) [below=0.35cm of n0-2,align=center] {first event of possible occurrence\\(\emph{initialized}, \emph{begins\_at})};
\draw [->] (initializedtxt) -- (n0-2);

\slidingwindowthingy{-2}{1}
\draw (-0.5*\interdotdistance,-2-0.5*\slidingwindowheight) -- ++(6*\interdotdistance,0) -- ++(0,\slidingwindowheight) -- ++(-6*\interdotdistance,0);
\node [right=10pt of dr1,align=left] {encountered $ A $;\\advance automaton to state 2,\\wait for C};

\slidingwindowthingy{-4}{2}
\draw (-0.5*\interdotdistance,-4-0.5*\slidingwindowheight) rectangle ++(8*\interdotdistance,\slidingwindowheight);
\node [right=10pt of dr2,align=left] {encountered $ C $;\\advance automaton to state 3,\\episode successfully recognized};
\node (marktxt) [above=0.35cm of n2-0,align=center,xshift=6pt] {save location of sliding\\window (\emph{in\_window})};
\draw [->] (n2-0 |- marktxt.south) -- (n2-0);

\slidingwindowthingy{-6}{3}
\draw ({(9.5)*\interdotdistance},-6-0.5*\slidingwindowheight) -- ++(-7*\interdotdistance,0) -- ++(0,\slidingwindowheight) -- ++(7*\interdotdistance,0);
\node [right=10pt of dr3,align=left] {occurrence no longer in window;\\determine number of windows\\which contained occurrence};

\draw (n3-0.south) -- ++(0,-6pt) -- node (numwindowsindicator) [midway,below,align=center] {displacement of window\\since recognition} ++(3*\interdotdistance,0) -- (n3-3.south);

\end{tikzpicture}

\caption{Illustration for the recognition of serial episode $ B \to A \to C $. Black dots are timestamps which don't contain $ B $, $ A $ or $ C$.}
\label{fig:serial-recognition}
\end{figure}

\subsubsection{Data structures}

The algorithm uses some bookkeeping data structures, which get updated as the sequence gets read. We'll discuss the most important ones.
\begin{itemize}
\item \textbf{waits} maps an event type to a set of pairs of the form $ (\alpha, j) $, where $ \alpha $ is an episode and $ j $ represents a state in the episode's automaton. If a pair $ (\alpha, j) $ is in $ \text{waits}(A) $, and $ j > 1 $, then $ \alpha $ has an instance of its automaton currently in state $ (j - 1) $ and is waiting for an event of type $ A $ to advance to the next state. Throughout the iteration over the sequence, $ \text{waits}(\alpha[1]) $ will always contain $ (\alpha, 1) $ for each episode $ \alpha $, since a new automaton instance should be instantiated any time the first event of a serial episode occurs.
\item \textbf{begins\_at} maps a timestamp to a set of pairs $ (\alpha, j) $. If $ (\alpha, j) $ is in $ \text{begins\_at}(t) $, then $ \alpha $ has an instance of its automaton in state $ j $, and it was initialized at timestamp $ t $.
\item Each episode $ \alpha $ has an $ | \alpha | $-element array called \textbf{initialized}, where $ \alpha \text{.initialized}[j] $ contains the timestamp in the sequence at which the instance currently in state $ j $ was initialized. If for a certain state there is currently no active instance, its corresponding element in the initialized array will be some special value---\cite{winepi97} chose 0, which we changed to \emph{uninitialized} for clarity and such that 0 can be a valid timestamp if necessary.

Note that \emph{initialized} allows at most one automaton to be in a given state at any time. This approach works because it is not useful to have multiple instances in the same state. If one automaton instance reaches a common state with another instance, they will simply make transitions simultaneously until the earlier instance gets removed. It suffices to maintain the instance which was reached the common state last, since it was initialized at a later timestamp, and thus will be the last to be removed.
\item All of the state transitions to be performed for a given timestamp are collected in a list, \textbf{transitions}, before they actually get executed. If they were executed immediately, an automaton could be incorrectly overwritten if there there are multiple events in a single timestamp. We will come back to this later.
\end{itemize}

\subsubsection{Operation}

With the data structures just described in mind, we will go through the algorithm in detail to see how it operates.

The initialization is rather straightforward. One thing to note is that the permanent pairs of the \emph{waits} sets are constructed (line~\ref{alglin:rec-ser-fwi:fill-waits-init}). As stated before, for each episode $ \alpha $, $ \text{waits}(\alpha[1]) $ contains $ (\alpha, 1) $ from the start, and throughout the algorithm, to initialize a new automaton instance for $ \alpha $ any time an event of type $ \alpha[1] $ occurs.

Then a sliding window is passed over the sequence, in the same manner algorithm~\ref{alg:rec-par-fwi} does (line~\ref{alglin:rec-ser-fwi:iterate-sequence}). Each time the sliding window is advanced, first the events that just came into the window are processed (line~\ref{alglin:rec-ser-fwi:iterate-new-events}), at time $ (\text{start} + \text{win} - 1) $. Say an event of type $ A $ came in. Thanks to the \emph{waits} data structure, all automaton instances that should make a state transition are found efficiently. For each of the pairs $ (\alpha, j) $ in $ \text{waits}(A) $, the following steps are evaluated:

\begin{enumerate}
\item (Lines~\ref{alglin:rec-ser-fwi:mark-enter} and further) If the automaton instance has reached state $ | \alpha | $, that is, the episode now occurs within the sliding window\footnote{This is not entirely correct. Note that new events coming into the window are processed before old events are removed. After processing new events, the oldest possible automata were therefore initialized at timestamp $ \text{start} - 1 $, which may cause an episode to be recognized without being in the current window $ s[\text{start}, \text{start} + \text{win}] $. However, for such a falsely recognized episode, the corresponding automaton will be removed immediately afterwards, and the displacement of the sliding window will be zero, thus not affecting $ \alpha $'s frequency count.}, then $ \alpha \text{.in\_window} $ gets set to \emph{start} to mark the time at which the episode entered the sliding window. Except if another instance is already in state $ | \alpha | $, then $ \alpha \text{.in\_window} $ was already set. If we would overwrite the value in that case, the windows covering the previous occurrence would not be counted.
\item (Lines~\ref{alglin:rec-ser-fwi:add-to-transitions} and further) The transition to be made is captured in the list of transitions.
\item (Lines~\ref{alglin:rec-ser-fwi:cleanup-old-states} and further) The old state information is removed from data structures \emph{begins\_at}, \emph{initialized}, and \emph{waits}, if we're not initializing a new automaton instance.
\end{enumerate}

When all new events have been processed, the transitions are finalized: the new state information is written to \emph{initialized}, \emph{begins\_at}, and \emph{waits}.

After applying the state transitions, the events no longer in the window are dropped. This entails looking up all automata which were instantiated at timestamp $ (\text{start} - 1) $ using the \emph{begins\_at} data structure (line~\ref{alglin:rec-ser-fwi:cleanup-iteration-begin} and further):

\begin{enumerate}
\item If the automaton instance was in state $ | \alpha | $, the episode occurred within the window until now. When the episode was first recognized to occur in the sliding window, the location of the sliding window was stored in $ \alpha \text{.in\_window} $, and so the number of fixed windows in which the episode occurred can be determined by the displacement of the sliding window since then.
\item If the automaton instance was not in state $ | \alpha | $, the \emph{waits} data structure is updated to reflect the removal of the instance.
\item The \emph{initialized} array of the episode is updated to reflect the removal of the automaton instance.
\end{enumerate}

\subsubsection{Correcting the original algorithm pseudocode}

While implementing this algorithm we found an error in the pseudocode, related to multiple instances of an episode's automaton reaching a common state. At line~\ref{alglin:rec-ser-fwi:transition-begin}, a transition of an automaton instance is being applied. The \emph{initialized} array gets updated, potentially overwriting a previous instance in the same state. \emph{begins\_at} gets updated with the new state, but the information about the potentially overwritten instance is not removed from \emph{begins\_at}. We'll illustrate how this affects the output with an example.

Using a window width of 2, and scanning the subsequence $ \cdots E \: A \: E \: C \cdots $. Say that the first $ E $ has timestamp $ t_1 $. Consider the recognition of the serial episode $ \alpha = E \to C $. Obviously the subsequence contains $ \alpha $, and a window width of 2 suffices to recognize an instance of the episode in the subsequence. Upon encountering the first $ E $, an automaton instance gets initialized for $ \alpha $ (lines \ref{alglin:rec-ser-fwi:transition-begin} through \ref{alglin:rec-ser-fwi:transition-end}). This includes, as discussed before:
\begin{itemize}
\item setting the timestamp in the \emph{initialized} array;
\item adding $ (\alpha, 2) $ to the $ \text{waits}(C) $ set (so that if $ C $ is encountered, the automaton can transition to the next state);
\item adding $ (\alpha, 1) $ to $ \text{begins\_at}(t_1) $ (to facilitate the removal of the the automaton instance once $ t_1 $ falls out of the window).
\end{itemize}

When the second $ E $ is read (at $ t = t_1 + 2 $ and when $ \text{start} = t_1 + 1 $), a new instance of the automaton is created, and since the previous instance is still in state $ 1 $---no $ C $ has been encountered---the previous instance must be overwritten. Indeed, this happens for the \emph{initialized} data structure at line~\ref{alglin:rec-ser-fwi:transition-begin}. However, \emph{begins\_at} is never updated to reflect this: $ (\alpha, 1) $ remains in $ \text{begins\_at}(t_1) $. Because of this, the newly created automaton is wrongly removed just afterwards, because the first instance fell out of the window (while iterating $ \text{begins\_at}(t_1 = \text{start} - 1) $, line~\ref{alglin:rec-ser-fwi:cleanup-for}). Then $ (\alpha, 2) $ is no longer in $ \text{waits}(C) $, and $ \alpha $ will fail to be recognized.

This is a problem any time an older instance of an episode's automaton instance gets overwritten by a more recent one reaching the same state. In some cases, as illustrated above, an occurrence of an episode may fail to be recognized entirely, while in other cases it may get recognized but with an incorrect frequency count, again due to an automaton instance being removed early. Consequently, this may cause the episode to be considered infrequent while it actually is frequent.

\subsubsection{The necessity of the \emph{transitions} data structure}

Earlier we mentioned that the \emph{transitions} data structure, which captures all transitions to execute at a timestamp before finalizing them, exists for good reason. In short, an automaton instance in state $ j $ could incorrectly overwrite an instance in state $ j + 1 $ if both automata are to transition. Consider the following example. Say $ \alpha = A \to B \to C $ is in the collection of candidates, and consider the event sequence $ \langle (A, 1),\allowbreak(B, 2),\allowbreak(A, 3),\allowbreak(B, 4),\allowbreak(C, 4) \rangle $. Note that the last two events occur at the same timestamp. Assume the window width is at least 4 so that it is sufficiently large to recognize $ \alpha $. When $ \text{start} + \text{win} - 1 = 1 $, a new automaton for $ \alpha $ gets instantiated.

% TODO finish the example

\subsection{Recognizing parallel episodes with minimal windows}

\begin{algorithm}

\caption{Recognizing a collection $ \mathcal{C} $ of parallel episodes in a sequence $ s $ using the minimal window frequency measure. \\
Input: A collection $ \mathcal{C} $ of parallel episodes, an event sequence $ \boldsymbol{s} = (s, T_s, T_e) $, a window width \textit{win}, and a frequency threshold \textit{min\_fr}. \\
Output: The episodes of $ \mathcal{C} $ that are frequent in $ \boldsymbol{s} $ with respect to \textit{win} and \textit{min\_fr}.}

\begin{algorithmic}[1]

\LineComment{Initialization}
\ForAll{event types $ A $}
    \State $ \text{in\_window}(A) \gets \text{empty queue} $
\EndFor
\ForAll{$ \alpha $ in $ \mathcal{C} $}
    \ForAll{$ A $ in $ \alpha $}
        \State $ \text{consider\_max}(\alpha, A) \gets 0 $
        \State $ \text{num\_needed}(\alpha, A) \gets \text{number of elements of type $ A $ in $ \alpha $} $
        \State $ \text{contains}(A) \gets \text{contains}(A) \cup \{ \alpha \} $
    \EndFor
\EndFor

\LineComment{Recognition}
\For{$ \text{start} \gets T_s - \text{win} + 1 $ to $ T_e - 1 $} % TODO check if starting at correct position
    \LineComment{Drop out old events from the window}
    \ForAll{events $ (A, t) $ in $ s $ such that $ t = \text{start} - 1 $}
        \While{$ | \text{in\_window}(A) | > 0 \wedge \text{in\_window}(A) \text{.front}() < \text{start} $}
            \State $ \text{in\_window}(A) \text{.pop}() $
        \EndWhile
    \EndFor

    \LineComment{Bring in new events to the window}
    \ForAll{events $ (A, t) $ in $ s $ such that $ t = \text{start} + \text{win} - 1 $}
        \State $ \text{in\_window}[A] \text{.push}(t + \text{win} - 1) $
        \ForAll{$ \alpha $ in $ \text{contains}(A) $}
            \If{{\footnotesize $ \forall A $ in $ \alpha : \min(| \text{in\_window}(A) |, \text{consider\_max}(\alpha, A)) \geq \text{num\_needed}(\alpha, A) $ }}
                \LineComment{Occurrence detected; determine start of minimal window}
                \State $ Q \gets \text{in\_window}(A) $
                \State $ \text{window\_start} \gets \min\{ t | \forall A $ in $ \alpha : t = Q[| Q | - \text{num\_needed}(\alpha, A) + 1] \} $
                \ForAll{$ A $ in $ \alpha $}
                    $ Q \gets \text{in\_window}(A) $
                    \If{$ \text{window\_start} = Q[| Q | - \text{num\_needed}(\alpha, A)) + 1] $}
                        \State $ \text{consider\_max}(\alpha, A) \gets \text{num\_needed}(\alpha, A) - 1 $
                    \EndIf
                    \State append $ [\text{window\_start}, \text{start} + \text{win}) $ to $ \alpha \text{.minimal\_windows} $
                \EndFor
            \EndIf
        \EndFor
    \EndFor
\EndFor

\end{algorithmic}

\end{algorithm}

With minimal windows, the frequency of an episode isn't expressed in terms of a number of windows anymore, but in the number of actual occurrences of the episode. For the fixed window frequency measure, % TODO finish sentence (and by extension, thesis)

\subsection{Removing episodes which have been found infrequent}
\label{sec:maintain-blocks}

\begin{algorithm}

\caption{Removing infrequent episodes from a collection of candidates $ \mathcal{C} $ for which \emph{freq\_count} is known. \\
Input: A sorted array of candidates $ \mathcal{C} $, including their \emph{block\_start} values, and their \emph{freq\_count} values with respect to some sequence, and a minimum frequency threshold \emph{min\_fr}. \\
Output: A sorted array $ \mathcal{F} $ of those episodes in $ \mathcal{C} $ which are frequent, along with consistent \emph{block\_start} values.
}

\begin{algorithmic}[1]

\State $ \text{new\_block\_start} \gets 1 $
\State $ \mathcal{F} \gets \text{empty array} $
\State $ \mathcal{F} \text{.block\_start} \gets \text{empty array} $
\For{$ i = 1 $; $ i \leq | \mathcal{C} | $; $ i \gets i + 1 $}
    \If{$ \alpha \text{.freq\_count} < \text{min\_fr} $}
        \LineComment{Episode infrequent; discard}
        \State continue with next i
    \EndIf
    \If{$ \mathcal{C} \text{.block\_start}[i] = i $} \label{alglin:remove-infrequent-episodes:different-block-test}
        \LineComment{Encountered new block in candidates}
        \State $ \text{new\_block\_start} \gets | \mathcal{F} | $
    \EndIf
    \State append $ \alpha $ to $ \mathcal{F} $
    \State $ \mathcal{F} \text{.block\_start}[ | \mathcal{F} | ] \gets \text{new\_block\_start} $
\EndFor
\State output $ \mathcal{F} $

\end{algorithmic}

\label{alg:remove-infrequent-episodes}
\end{algorithm}

At first sight, it seems a trivial task to discard episodes from a list of candidates which have turned out to be infrequent. But one thing needs to be taken into consideration, namely the auxiliary \emph{block\_start} variables, used in the candidate generation algorithm (section~\ref{sec:cand-gen}). Each episode $ \alpha $ has such a \emph{block\_start} value, which denotes the array index of the first episode in the list for which it shares the first $ \alpha - 1 $ elements in the episode's array representation. If we remove episodes without further consideration, these indices will be invalidated. They should not be invalidated, however, since the next round of the candidate generation step relies on them. Hence we need to keep track of the blocks while constructing the list of frequent episodes. Algorithm~\ref{alg:remove-infrequent-episodes} achieves this.

The algorithm stores the frequent episodes in a new data structure $ \mathcal{F} $. While iterating over the episodes in order, the array index of the new block start is kept. If for an index $ i $, $ \mathcal{C}. \text{block\_start}[i] = i $, we know that a new block started at position $ i $ in $ \mathcal{C} $, and so \emph{new\_block\_start} gets updated in order to start a new block in $ \mathcal{F} $ as well.
