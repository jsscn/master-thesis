\chapter{Experiments}
\label{sec:experiments}

A big point of comparison while implementing our algorithms was a closed episode miner\footnote{http://adrem.ua.ac.be/mining-closed-strict-episodes}. It allowed us to verify the correctness of our implementation. The closed episode miner differs in a few ways from our implementation:

\begin{itemize}
\item It generates general episodes, not just parallel and serial episodes.
\item It generates closed episodes. Closed episodes are episodes that have no superepisodes of the same frequency. Mining only closed episodes helps in reducing the amount of output. The implementation has options to mine non-closed episodes as well, though, which allows us to compare with our implementation.
\item While our implementation finds episodes of one class at a time, as specified by the user---parallel or serial---the closed episode miner finds episodes of all classes at once---parallel, serial and general.
\end{itemize}

Given the above, in order to compare the two implementations in terms of their output, we have to enable the options that cause the closed episode miner to find non-closed episodes, since our implementation finds non-closed episodes as well; and we have to filter out those episodes which don't match the class of episodes we're currently mining.

Mining general episodes is of higher complexity than parallel and serial episodes.

Parallel episodes can only ``grow'' by adding a new node, and serial episodes grow in much the same way: by adding a node and an edge at the same time. Pattern explosion is a bigger concern with general episodes: additionally, they can grow by adding only an edge. So, ideally, our implementation should be faster than the closed episode miner.

% ^ terrible ^

\section{Datasets}

We will conduct experiments using a number of datasets:

\begin{itemize}
\item \emph{abstract}: a dataset consisting of the first 739 NSF award abstracts from 1990, merged into one long sequence\footnote{\url{http://kdd.ics.uci.edu/databases/nsfabs/nsfawards.html}}.
\item \emph{tolstoy}: Leo Tolstoy's novel Anna Karenina, from Project Gutenberg\footnote{\url{https://www.gutenberg.org/ebooks/1399}}.
\item \emph{trains}: a dataset consisting of departure times of delayed trains in a Belgian railway station, for trains with a delay of at least three minutes. This data is anonymized, so we won't be able derive any meaning from the patterns, but it it an interesting dataset nonetheless, because contrary to the textual datasets, \emph{trains} is sparse.
\end{itemize}

The textual datasets were preprocessed by lemmatizing using the Porter stemmer\footnote{\url{https://tartarus.org/martin/PorterStemmer}} and stop words were removed.

Table~\ref{table:datasets-numbers} shows some statistics about each dataset, where $ | \Sigma | $ is the size of the alphabet, $ | s | $ is the number of events, and $ T_e - T_s $ is the time range of the sequence. For dense sequences, $ T_e - T_s = | s | $. For sparse sequences, a window of $ \rho $ contains on average $ | s | \frac\rho{T_e - T_s} $ events.

\begin{table}
\centering

\begin{tabulary}{\textwidth}{ L|RRRC }

dataset & \multicolumn{1}{c}{$ | \Sigma | $} & \multicolumn{1}{c}{$ | s | $} & \multicolumn{1}{c}{$ T_e - T_s $} & type \\
\hline
\emph{abstract} & $ 51\,346 $ & $ 67\,828 $ & $ 67\,828 $ & dense \\
\emph{tolstoy} & $ 95\,623 $ & $ 124\,627 $ & $ 124\,627 $ & dense \\
\emph{trains} & $ 7\,874 $ & $ 10\,115 $ & $ 26\,626\,667 $ & sparse \\

\end{tabulary}

\caption{Some properties of the datasets $ (s, T_s, T_e) $.}
\label{table:datasets-numbers}
\end{table}

We see that the textual and \emph{trains} datasets have quite large alphabets. Figures~\ref{fig:frequency-plot-nsf} and \ref{fig:frequency-plot-nsf} and \ref{fig:frequency-plot-tolstoy} show the number of occurrences of the most frequent event types, ordered by frequency. We observe a long tail\footnote{\url{https://en.wikipedia.org/wiki/Long_tail}} for \emph{abstract} and \emph{tolstoy}, which is common for natural-language texts. In \emph{trains}, too, there is a similar progression, with a small number of the event types occurring a significant number of times, and the vast majority of event types occurring very rarely. Note that the graphs don't even show all event types, although for \emph{abstract} and \emph{trains} the rightmost event types occur only once, and those for \emph{tolstoy} occur fewer than 10 times.

\begin{figure}
\centering

\def\testwidth{6cm}
\def\testheight{5cm}

\begin{subfigure}[b]{\textwidth}
\centering

\begin{tikzpicture}

\begin{axis}[
    width=\testwidth,
    height=\testheight,
    xlabel={event types ordered by frequency},
    ylabel={number of occurrences},
    % ymode=log
]

\addplot table [x=rank,y=count,mark=none] {experiments/nsf-alphabet-frequency-300.dat};

\end{axis}

\end{tikzpicture}

\caption{The frequency of the 300 most frequent events in \emph{abstract}.}
\label{fig:frequency-plot-nsf}
\end{subfigure}

\par\bigskip

\begin{subfigure}[b]{\textwidth}
\centering

\begin{tikzpicture}

\begin{axis}[
    width=\testwidth,
    height=\testheight,
    xlabel={event types ordered by frequency},
    ylabel={number of occurrences},
    % ymode=log
]

\addplot table [x=rank,y=count,mark=none] {experiments/tolstoy-alphabet-frequency-2200.dat};

\end{axis}

\end{tikzpicture}

\caption{The frequency of the 2200 most frequent events in \emph{tolstoy}.}
\label{fig:frequency-plot-tolstoy}
\end{subfigure}

\par\bigskip

\begin{subfigure}[b]{\textwidth}
\centering

\begin{tikzpicture}

\begin{axis}[
    width=\testwidth,
    height=\testheight,
    xlabel={event types ordered by frequency},
    ylabel={number of occurrences},
    % ymode=log
]

\addplot table [x=rank,y=count,mark=none] {experiments/trains-alphabet-frequency-300.dat};

\end{axis}

\end{tikzpicture}

\caption{The frequency of the 300 most frequent events in \emph{trains}.}
\label{fig:frequency-plot-trains}
\end{subfigure}

\caption{The frequency of the most frequent event types in the datasets.}
\label{fig:alphabet-frequencies}
\end{figure}


\section{Performance}
\label{sec:performance}

To assess the efficiency of the algorithm, we inspect the runtime for different input parameters: comparing both episode classes, the frequency measures, while varying the window width and the frequency and confidence thresholds.

The performance experiments were ran as follows. For specified episode classes, frequency measures, a list of window widths, and a range of frequency thresholds, an experiment would run the cartesian product of all these parameters, within time and memory constraints. A few particularities:

\begin{itemize}
\item The range of frequency thresholds has an exponentially decreasing nature: it is specified by a (high) starting threshold, a multiplier $ \in (0, 1) $, and a lower bound. Each iteration, the current frequency threshold is multiplied with the multiplier to obtain the next frequency threshold. For example, with a multiplier of 0.90, the next threshold is always 10\% smaller than the last.
\item For each combination of episode class, frequency measure, and window width, a thread is run with progressively lower frequency thresholds, as described above. If memory runs out or the timeout is exceeded before reaching the lower bound, all lower frequency thresholds for that combination of episode class, frequency measure, and window width are skipped, as they will take at least as much time and memory as the current threshold.
\end{itemize}

All performance experiments were run on the same machine; the full specifications of which can be found online\footnote{The specifications can be found at \url{https://support.apple.com/kb/sp623}. 2.7 GHz model; memory manually upgraded to 12~GB. Running macOS 10.13.5. All C++ code compiled with clang-900.0.39.2 from LLVM~9.0. All Java code run with Java~SE~1.8.}.


\subsection{Episodes}
\label{sec:performance-episodes}


\iffalse
% --- Plotting the number of candidates and the number of frequent episodes. Maybe not so useful.
\begin{tikzpicture}

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=outer north east},
    xlabel={number of candidates},
    ylabel={number of frequent episodes},
]

\addplot table [x=num-frequent-episodes,y=num-candidates] {experiments/nsf/nsf-parallel-fixed-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=num-candidates] {experiments/nsf/nsf-parallel-minimal-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=num-candidates] {experiments/nsf/nsf-parallel-weighted-windows-8.tsv};

\end{axis}

\end{tikzpicture}

\begin{tikzpicture}

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=outer north east},
    xlabel={number of candidates},
    ylabel={number of frequent episodes},
]

\addplot table [x=num-frequent-episodes,y=percentage-frequent-of-candidates] {experiments/nsf/nsf-parallel-fixed-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=percentage-frequent-of-candidates] {experiments/nsf/nsf-parallel-minimal-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=percentage-frequent-of-candidates] {experiments/nsf/nsf-parallel-weighted-windows-8.tsv};

\end{axis}

\end{tikzpicture}

\begin{tikzpicture}

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=outer north east},
    xlabel={number of candidates},
    ylabel={number of frequent episodes},
]

\addplot table [x=num-candidates,y=num-frequent-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=num-candidates,y=num-frequent-episodes] {experiments/trains/trains-parallel-minimal-windows-900.tsv};
\addplot table [x=num-candidates,y=num-frequent-episodes] {experiments/trains/trains-parallel-weighted-windows-900.tsv};

\end{axis}

\end{tikzpicture}
% --- ---
\fi

\subsection{Association rules}

Figure~\ref{fig:runtimes-rules-trains-900} shows runtimes for generating association rules from \emph{trains} using different episode classes, as a function of the number of frequent episodes. Some weird results though % TODO fix

As we saw in algorithm~\ref{alg:association-rules-top-level}, for each frequent episode $ \beta $, all $ \alpha \Rightarrow \beta $ with $ \alpha \subset \beta $ are considered. In our experiments, the association mining process usually took a small fraction of the duration of the episode mining process. But, assuming an injective $ lab $-function (an event type appears at most once), the number of subepisodes of an episode is exponential in the size of the episode. So, when there was many large episodes, mining association rules started to take significant time.

\begin{figure}

\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=north west},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
]

\addplot table [x=num-frequent-episodes,y=duration-rules-0.0] {experiments/nsf/nsf-parallel-fixed-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-rules-0.0] {experiments/nsf/nsf-parallel-minimal-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-rules-0.0] {experiments/nsf/nsf-parallel-weighted-windows-8.tsv};

\end{axis}

\end{tikzpicture}

\caption{rules consisting of parallel episodes}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=north west},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
]

\addplot table [x=num-frequent-episodes,y=duration-rules-0.0] {experiments/nsf/nsf-serial-fixed-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-rules-0.0] {experiments/nsf/nsf-serial-minimal-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-rules-0.0] {experiments/nsf/nsf-serial-weighted-windows-8.tsv};

\end{axis}

\end{tikzpicture}

\caption{rules consisting of serial episodes}
\end{subfigure}

\caption{Runtimes for finding association rules from episodes generated from \emph{trains} (same experiment as figure~\ref{fig:runtimes-trains-900}) for different confidence thresholds (0.0, 0.4 and 1.0).}
\label{fig:runtimes-rules-trains-900}
\end{figure}

\subsection{Number of candidate episodes versus number of frequent episodes}

The database passes are an expensive---if not the most expensive---part of the mining algorithm. Plotting the number of frequent episodes in the output and the number of candidates that were considered in a pass over the sequence gives us an idea of the amount of internal work the algorithms deal with for the output that gets produced.

\subsection{Comparing the frequency measures}

We would like to compare the efficiency for the different frequency measures across a range of frequency thresholds. However we should not evaluate the runtimes as a function of the frequency threshold directly, since the values for each of the measures are semantically different. For instance, the weighted-window frequency of an episode in a sequence is at most equal to the disjoint-window frequency; so for otherwise equal parameters (including thresholds), the weighted-window frequency will produce a subset of the disjoint-window frequency. Instead we can compare runtimes as a function of the number of frequent episodes.

Figure~\ref{fig:runtimes-nsf-8} shows such plots for both parallel and serial episodes, using the \emph{abstracts} dataset. We see that the fixed-window frequency performs significantly better than the other measures for parallel episodes. The weighted-window frequency runs up against the timeout very quickly in comparison.

For serial episodes, the fixed-window frequency has less of an advantage, and at a certain point the minimal-window frequency produces more episodes for in less time. This could be explained by the fact that the data pass algorithm that finds minimal windows of serial episodes is slightly simpler than the algorithm that determines the fixed-window frequency for serial episodes. While the weighted-window frequency algorithm uses the same data pass algorithms as the disjoint-window frequency, selecting the optimal selection of non-overlapping windows is more complex, explaining its higher runtimes.

\begin{figure}

\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=south east},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
]

\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-parallel-fixed-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-parallel-minimal-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-parallel-weighted-windows-8.tsv};

\end{axis}

\end{tikzpicture}

\caption{parallel episodes}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=south east},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
]

\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-serial-fixed-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-serial-minimal-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-serial-weighted-windows-8.tsv};

\end{axis}

\end{tikzpicture}

\caption{serial episodes}
\end{subfigure}

\caption{Runtimes for finding episodes in dataset \emph{abstract} using a window width of 8.}
\label{fig:runtimes-nsf-8}
\end{figure}

\begin{figure}

\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=north west},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
    xmode=log,
    ymode=log,
]

\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/trains/trains-parallel-minimal-windows-900.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/trains/trains-parallel-weighted-windows-900.tsv};

\end{axis}

\end{tikzpicture}

\caption{parallel episodes}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=north west},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
    xmode=log,
    ymode=log,
]

\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/trains/trains-serial-fixed-windows-900.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/trains/trains-serial-minimal-windows-900.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/trains/trains-serial-weighted-windows-900.tsv};

\end{axis}

\end{tikzpicture}

\caption{serial episodes}
\end{subfigure}

\caption{Runtimes for finding episodes in dataset \emph{trains} using a window width of 8.}
\label{fig:runtimes-trains-900}
\end{figure}



\subsection{Comparing sequence lengths}

\begin{figure}

\begin{subfigure}[b]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend pos=north west,
    legend entries={fixed windows,minimal windows,weighted windows},
    xlabel={portion of the whole sequence},
    ylabel={runtime (s)},
    % ymode=log,
]

\addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-parallel-fixed-windows.dat};
\addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-parallel-minimal-windows.dat};
\addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-parallel-weighted-windows.dat};
% \addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-serial-fixed-windows.dat};
% \addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-serial-minimal-windows.dat};
% \addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-serial-weighted-windows.dat};

\end{axis}

\end{tikzpicture}
\caption{linear scale}
\label{fig:tolstoy-runtime-vs-length-linear-scale}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend pos=south east,
    legend entries={fixed windows,minimal windows,weighted windows},
    xlabel={portion of the whole sequence},
    ylabel={runtime (s)},
    ymode=log,
]

\addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-parallel-fixed-windows.dat};
\addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-parallel-minimal-windows.dat};
\addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-parallel-weighted-windows.dat};
% \addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-serial-fixed-windows.dat};
% \addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-serial-minimal-windows.dat};
% \addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-serial-weighted-windows.dat};

\end{axis}

\end{tikzpicture}
\caption{logaritmic scale}
\label{fig:tolstoy-runtime-vs-length-log-scale}
\end{subfigure}

\caption{Runtimes for different portions of \emph{tolstoy} using a fixed frequency threshold for each frequency measure. $ \rho = 15 $.}
\label{fig:tolstoy-runtime-vs-length}
\end{figure}

In sequential pattern mining, sequences are expected to be very long. Therefore we would like to have an idea of how the length of the sequence affects the performance. We measured the runtime of the algorithm for different-sized prefixes of the \emph{tolstoy} dataset for all episode classes and frequency measures. The results can be seen in figure~\ref{fig:tolstoy-runtime-vs-length}. The general progression is fairly similar for all of the measures. While unfortunately the progression is not linear (figure~\ref{fig:tolstoy-runtime-vs-length-linear-scale}) it doesn't seem to be exponential---if that were the case, we would see straight lines on a graph with a logarithmic scale (figure~\ref{fig:tolstoy-runtime-vs-length-log-scale}).

\subsection{Comparing performance with the closed episode miner}

Alongside the performance experiments, we ran the closed episode miner on some of the same parameter configurations.

% TODO do some runs and write about them (refer back to earlier experiments)

\section{Correctness}

Throughout implementing our episode miner, we validated the output of our implementation with that of the closed episode miner, automatically running the closed episode miner alongside our implementation, then parsing and comparing its results. Thanks to this closed episode miner we were able to solve quite a few errors in our implementation. We did find, however, a possible bug in the closed episode miner.

When mining parallel episodes in the example sequence (figure~\ref{fig:event-sequence}), using the weighted-window frequency and with a window width of 8, the frequencies for a few episodes differed, as shown in table~\ref{table:closepi-frequency-difference}. Observing the sequence, each of these episodes has two overlapping minimal windows, of which the second one has a greater weight. Our implementation seems to correctly select the window with the higher weight, while the closed episode miner seems to choose the first window.

Further distilling the issue, we tested a simpler example: the sequence $ \langle (a, 1), (a, 3), (a, 4) \rangle $, episode $ \{ a, a \} $, and a window width of at least 3. The weighted-window frequency is clearly $ 1 / 2 $, being the weight of the minimal window $ [3, 5) $, but the closed episode miner reported $ 1 / 3 $, so it undisputably selected $ [1, 4) $.

\begin{table}
\centering

\begin{tabulary}{\textwidth}{ C|C|C }

$ \alpha $ & $ fr_w(\alpha) $ (ours) & $ fr_w(\alpha) $ (closed episode miner) \\
\hline
$ \{ b, d \} $ & $ 0.2 $ ($ 1/5 $) & $ 0.166 \ldots $ ($ 1/6 $) \\
$ \{ a, b, d \} $ & $ 0.2 $ ($ 1/5 $) & $ 0.142857 \ldots $ ($ 1/7 $) \\
$ \{ a, b, e \} $ & $ 0.25 $ ($ 1/4 $) & $ 1.66 \ldots $ ($ 1/6 $) \\

\end{tabulary}

\caption{Differing weighted-window frequency values between two implementations, mining the example sequence from figure~\ref{fig:event-sequence}.}
\label{table:closepi-frequency-difference}
\end{table}

\section{Quality}

In this section, we will do a qualitative analysis. We compare the top-ranked results across the different frequency measures we implemented, and those of cohesion-based interestingness measures, as we will explain later.


\iffalse
\subsection{Comparing the frequency measures on a toy example}

We consider the example sequence of the example in figure~\ref{fig:event-sequence}, which was used as an example throughout chapter~\ref{sec:problem-statement}. A small sequence is interesting to analyze because we have a full overview of the dataset, and can therefore provide insight into how the frequency and confidence values came to be. Also, it is possible to generate all episodes that cover the sequence for a certain window size, using a low frequency thresold.

We'll generate all episodes using all of the frequency measures we implemented.
\fi

% TODO not do?

\subsection{Analysis of episodes mined from \emph{tolstoy} dataset}

\begin{table}

\begin{tabulary}{\textwidth}{R|L|L|L}

\# & fixed-window fr. & disjoint-window fr. & weighted-window fr. \\
\hline
1 & $ \{ \text{levin} \} $ (20913) & $ \{ \text{levin} \} $ (1629) & $ \{ \text{levin} \} $ (1629) \\
2 & $ \{ \text{vronski} \} $ (11165) & $ \{ \text{vronski} \} $ (865) & $ \{ \text{vronski} \} $ (865) \\
3 & $ \{ \text{anna} \} $ (10699) & $ \{ \text{anna} \} $ (823) & $ \{ \text{anna} \} $ (823) \\
4 & $ \{ \text{thought} \} $ (8994) & $ \{ \text{kitti} \} $ (672) & $ \{ \text{kitti} \} $ (672) \\
5 & $ \{ \text{time} \} $ (8948) & $ \{ \text{thought} \} $ (663) & $ \{ \text{thought} \} $ (663) \\
6 & $ \{ \text{kitti} \} $ (8826) & $ \{ \text{time} \} $ (651) & $ \{ \text{time} \} $ (651) \\
7 & $ \{ \text{hand} \} $ (8645) & $ \{ \text{hand} \} $ (651) & $ \{ \text{hand} \} $ (651) \\
8 & $ \{ \text{alexei} \} $ (8619) & $ \{ \text{smile} \} $ (632) & $ \{ \text{smile} \} $ (632) \\
9 & $ \{ \text{smile} \} $ (8549) & $ \{ \text{alexei} \} $ (632) & $ \{ \text{alexei} \} $ (632) \\
10 & $ \{ \text{face} \} $ (8315) & $ \{ \text{face} \} $ (598) & $ \{ \text{face} \} $ (598) \\
11 & $ \{ \text{ey} \} $ (8062) & $ \{ \text{love} \} $ (595) & $ \{ \text{love} \} $ (595) \\
12 & $ \{ \text{alexandrovitch} \} $ (7842) & $ \{ \text{alexandrovitch} \} $ (571) & $ \{ \text{alexandrovitch} \} $ (571) \\
13 & $ \{ \text{felt} \} $ (7753) & $ \{ \text{alexei},\allowbreak\text{alexandrovitch} \} $ (571) & $ \{ \text{ey} \} $ (570) \\
14 & $ \{ \text{man} \} $ (7751) & $ \{ \text{ey} \} $ (570) & $ \{ \text{man} \} $ (565) \\
15 & $ \{ \text{feel} \} $ (7596) & $ \{ \text{man} \} $ (565) & $ \{ \text{feel} \} $ (561) \\

\end{tabulary}

\caption{The top 15 parallel episodes found by our algorithm, with $ \rho = 15 $, and for the three frequency measures.}
\label{table:fmw-tolstoy-top-15-parallel-episodes}
\end{table}

As we would expect, if we rank the output by frequency, the top contains mostly just episodes of size 1 (table~\ref{table:fmw-tolstoy-top-15-parallel-episodes}). It does give us some information about the text, though not much more than when we simply count the occurrences of all words.

\begin{itemize}
\item We learn of many characters' names---either first or last, but we don't know many characters' first and last name.
\item Looking at the column for the disjoint-window frequency, we see that two names are mentioned together frequently: $ \{ \text{alexei}, \text{alexandrovitch} \} $. From this information it is likely that a character named \emph{Alexei Alexandrovitch} appears often in the book. (This is indeed the case.) Moreover, we see that $ \{ \text{alexei}, \text{alexandrovitch} \} $ is just as frequent as subepisode $ \{ \text{alexandrovitch} \} $. So wherever the first name \emph{Alexei} is mentioned, the last name \emph{Alexandrovitch} is mentioned nearby (within at most within 15 words).
\item Common words like \emph{thought}, \emph{smile}, \emph{face}, \emph{love}, \emph{eye} (stemmed to \emph{ey}), \emph{feel} can give some indication of genre. At least it seems clear that the sequence does not represent a research paper in computer science.

\end{itemize}

\begin{table}

\begin{tabulary}{\textwidth}{R|L|L|L}

\# & fixed-window fr. & disjoint-window fr. & weighted-window fr. \\
\hline
1 & $ \{ \text{alexei},\allowbreak\text{alexandrovitch} \} $ (7416) & $ \{ \text{alexei},\allowbreak\text{alexandrovitch} \} $ (571) & $ \{ \text{alexei},\allowbreak\text{alexandrovitch} \} $ (286) \\
2 & $ \{ \text{stepan},\allowbreak\text{arkadyevitch} \} $ (7117) & $ \{ \text{stepan},\allowbreak\text{arkadyevitch} \} $ (547) & $ \{ \text{stepan},\allowbreak\text{arkadyevitch} \} $ (274) \\
3 & $ \{ \text{sergei},\allowbreak\text{ivanovitch} \} $ (3763) & $ \{ \text{levin},\allowbreak\text{levin} \} $ (395) & $ \{ \text{sergei},\allowbreak\text{ivanovitch} \} $ (146) \\
4 & $ \{ \text{levin},\allowbreak\text{levin} \} $ (3348) & $ \{ \text{sergei},\allowbreak\text{ivanovitch} \} $ (291) & $ \{ \text{darya},\allowbreak\text{alexandrovna} \} $ (102) \\
5 & $ \{ \text{darya},\allowbreak\text{alexandrovna} \} $ (2739) & $ \{ \text{darya},\allowbreak\text{alexandrovna} \} $ (205) & $ \{ \text{levin},\allowbreak\text{levin} \} $ (61.6) \\
6 & $ \{ \text{levin},\allowbreak\text{kitti} \} $ (2039) & $ \{ \text{levin},\allowbreak\text{kitti} \} $ (202) & $ \{ \text{lidia},\allowbreak\text{ivanovna} \} $ (54) \\
7 & $ \{ \text{anna},\allowbreak\text{vronski} \} $ (1942) & $ \{ \text{stepan},\allowbreak\text{levin} \} $ (199) & $ \{ \text{anna},\allowbreak\text{vronski} \} $ (45.1) \\
8 & $ \{ \text{arkadyevitch},\allowbreak\text{levin} \} $ (1896) & $ \{ \text{arkadyevitch},\allowbreak\text{levin} \} $ (197) & $ \{ \text{smile},\allowbreak\text{levin} \} $ (41.3) \\
9 & $ \{ \text{stepan},\allowbreak\text{levin} \} $ (1887) & $ \{ \text{stepan},\allowbreak\text{arkadyevitch},\allowbreak\text{levin} \} $ (195) & $ \{ \text{levin},\allowbreak\text{kitti} \} $ (41.1) \\
10 & $ \{ \text{stepan},\allowbreak\text{arkadyevitch},\allowbreak\text{levin} \} $ (1784) & $ \{ \text{vronski},\allowbreak\text{vronski} \} $ (191) & $ \{ \text{room},\allowbreak\text{draw} \} $ (40.6) \\
11 & $ \{ \text{smile},\allowbreak\text{levin} \} $ (1778) & $ \{ \text{anna},\allowbreak\text{vronski} \} $ (180) & $ \{ \text{countess},\allowbreak\text{lidia} \} $ (39.4) \\
12 & $ \{ \text{vronski},\allowbreak\text{vronski} \} $ (1722) & $ \{ \text{smile},\allowbreak\text{levin} \} $ (171) & $ \{ \text{thought},\allowbreak\text{levin} \} $ (39.3) \\
13 & $ \{ \text{time},\allowbreak\text{levin} \} $ (1597) & $ \{ \text{anna},\allowbreak\text{anna} \} $ (170) & $ \{ \text{love},\allowbreak\text{love} \} $ (38.2) \\
14 & $ \{ \text{brother},\allowbreak\text{levin} \} $ (1558) & $ \{ \text{time},\allowbreak\text{levin} \} $ (159) & $ \{ \text{arkadyevitch},\allowbreak\text{levin} \} $ (37.2) \\
15 & $ \{ \text{anna},\allowbreak\text{anna} \} $ (1531) & $ \{ \text{good},\allowbreak\text{levin} \} $ (153) & $ \{ \text{agafea},\allowbreak\text{mihalovna} \} $ (37) \\

\end{tabulary}

\caption{The top 15 parallel episodes found by our algorithm, excluding 1-episodes, with $ \rho = 15 $, and for the three frequency measures.}
\label{table:fmw-tolstoy-top-15-parallel->1-episodes}
\end{table}

\begin{table}

\begin{tabulary}{\textwidth}{R|L|L|L}

\# & disjoint-window fr. & weighted-window fr. \\
\hline
1 & $ \text{alexei} \to \text{alexandrovitch} $ (7401) & $ \text{alexei} \to \text{alexandrovitch} $ (571) & $ \text{alexei} \to \text{alexandrovitch} $ (286) \\
2 & $ \text{stepan} \to \text{arkadyevitch} $ (7106) & $ \text{stepan} \to \text{arkadyevitch} $ (547) & $ \text{stepan} \to \text{arkadyevitch} $ (274) \\
3 & $ \text{sergei} \to \text{ivanovitch} $ (3758) & $ \text{levin} \to \text{levin} $ (395) & $ \text{sergei} \to \text{ivanovitch} $ (146) \\
4 & $ \text{levin} \to \text{levin} $ (3348) & $ \text{sergei} \to \text{ivanovitch} $ (291) & $ \text{darya} \to \text{alexandrovna} $ (102) \\
5 & $ \text{darya} \to \text{alexandrovna} $ (2734) & $ \text{darya} \to \text{alexandrovna} $ (205) & $ \text{levin} \to \text{levin} $ (61.6) \\
6 & $ \text{vronski} \to \text{vronski} $ (1722) & $ \text{vronski} \to \text{vronski} $ (191) & $ \text{lidia} \to \text{ivanovna} $ (54) \\
7 & $ \text{anna} \to \text{anna} $ (1531) & $ \text{anna} \to \text{anna} $ (170) & $ \text{draw} \to \text{room} $ (40.5) \\
8 & $ \text{lidia} \to \text{ivanovna} $ (1438) & $ \text{alexandrovitch} \to \text{alexei} $ (145) & $ \text{countess} \to \text{lidia} $ (39.1) \\
9 & $ \text{love} \to \text{love} $ (1411) & $ \text{kitti} \to \text{kitti} $ (144) & $ \text{love} \to \text{love} $ (38.2) \\
10 & $ \text{arkadyevitch} \to \text{levin} $ (1245) & $ \text{love} \to \text{love} $ (140) & $ \text{agafea} \to \text{mihalovna} $ (37) \\
11 & $ \text{kitti} \to \text{kitti} $ (1200) & $ \text{arkadyevitch} \to \text{levin} $ (137) & $ \text{levin} \to \text{felt} $ (32) \\
12 & $ \text{levin} \to \text{kitti} $ (1160) & $ \text{levin} \to \text{kitti} $ (136) & $ \text{vronski} \to \text{vronski} $ (31.3) \\
13 & $ \text{vronski} \to \text{anna} $ (1137) & $ \text{stepan} \to \text{levin} $ (132) & $ \text{good} \to \text{humor} $ (29.1) \\
14 & $ \text{levin} \to \text{felt} $ (1130) & $ \text{stepan} \to \text{arkadyevitch} \to \text{levin} $ (132) & $ \text{anna} \to \text{arkadyevna} $ (28.5) \\
15 & $ \text{draw} \to \text{room} $ (1126) & $ \text{kitti} \to \text{levin} $ (131) & $ \text{vronski} \to \text{anna} $ (28.1) \\

\end{tabulary}

\caption{The top 15 serial episodes found by our algorithm, excluding 1-episodes, with $ \rho = 15 $, and for the three frequency measures.}
\label{table:fmw-tolstoy-top-15-serial->1-episodes}
\end{table}

From studying table~\ref{table:fmw-tolstoy-top-15-all-episodes} it is clear that simply ranking episodes by frequency is not a good strategy for getting the most out of our algorithm. We should at least filter out the 1-episodes, as those don't give any more information than counting the support of each word that appears in the text. Table~\ref{table:fmw-tolstoy-top-15-parallel->1-episodes} and table~\ref{table:fmw-tolstoy-top-15-serial->1-episodes} show the rankings of greater-than-1-episodes, for parallel and serial episodes respectively. If we want to highlight even larger episodes, we can group episodes by size.

After removing 1-episodes we learn some more things:
\begin{itemize}
\item We find full names---Alexei Alexandrovitch, Stepan Arkadyevitch, Sergei Ivanovitch, Lidia Ivanovna, Darya Alexandrovna, Agafea Mihalovna---all are characters' full names. With serial episodes, we find their order as well---\emph{alexei} usually precedes \emph{alexandrovitch} closely, so $ \text{alexei} \to \text{alexandrovitch} $ is rated more highly than $ \text{alexandrovitch} \to \text{alexei} $.
\item We also find the most important couples, since naturally their names are often mentioned close to each other---Kitty and Levin, Anna and Vronski.
\item The weighted-window frequency finds $ \{ \text{draw}, \text{room} \} $ and $ \text{draw} \to \text{room} $, which isn't included in the top 15 of the other measures. The phrase may not be mentioned as often, but the weighted-window frequency ranks it highly because of the noun \emph{drawing room}, making for many minimal windows of great weight.
\end{itemize}

\section{Comparing the different confidence measures}

The algorithm always uses the associated frequency and confidence measures together. For instance, association rules mined by the disjoint-window frequency will generate association rules according to the minimal-window confidence.

\begin{table}

\begin{tabulary}{\textwidth}{R|L}

\# & weighted-window confidence \\
\hline
1 & $ \{ \text{arkadyevitch}, \text{arkadyevitch} \} \Rightarrow \{ \text{stepan}, \text{arkadyevitch}, \text{arkadyevitch} \} $ (1) \\
2 & $ \{ \text{alexandrovitch}, \text{alexandrovitch} \} \Rightarrow \{ \text{alexei}, \text{alexandrovitch}, \text{alexandrovitch} \} $ (1) \\
3 & $ \{ \text{stepan}, \text{stepan} \} \Rightarrow \{ \text{stepan}, \text{stepan}, \text{arkadyevitch} \} $ (1) \\
4 & $ \{ \text{countess}, \text{ivanovna} \} \Rightarrow \{ \text{countess}, \text{lidia}, \text{ivanovna} \} $ (0.943) \\
5 & $ \{ \text{arkadyevitch}, \text{alexei} \} \Rightarrow \{ \text{stepan}, \text{arkadyevitch}, \text{alexei} \} $ (0.937) \\
6 & $ \{ \text{stepan}, \text{arkadyevitch}, \text{alexandrovitch} \} \Rightarrow \{ \text{stepan}, \text{arkadyevitch}, \text{alexei}, \text{alexandrovitch} \} $ (0.937) \\
7 & $ \{ \text{stepan}, \text{alexei}, \text{alexandrovitch} \} \Rightarrow \{ \text{stepan}, \text{arkadyevitch}, \text{alexei}, \text{alexandrovitch} \} $ (0.935) \\
8 & $ \{ \text{stepan}, \text{alexandrovitch} \} \Rightarrow \{ \text{stepan}, \text{alexei}, \text{alexandrovitch} \} $ (0.934) \\
9 & $ \{ \text{stepan}, \text{alexandrovitch} \} \Rightarrow \{ \text{stepan}, \text{arkadyevitch}, \text{alexandrovitch} \} $ (0.934) \\
10 & $ \{ \text{stepan}, \text{good} \} \Rightarrow \{ \text{stepan}, \text{arkadyevitch}, \text{good} \} $ (0.933) \\
11 & $ \{ \text{arkadyevitch}, \text{alexandrovitch} \} \Rightarrow \{ \text{arkadyevitch}, \text{alexei}, \text{alexandrovitch} \} $ (0.933) \\
12 & $ \{ \text{stepan}, \text{alexei} \} \Rightarrow \{ \text{stepan}, \text{arkadyevitch}, \text{alexei} \} $ (0.931) \\
13 & $ \{ \text{arkadyevitch}, \text{alexei}, \text{alexandrovitch} \} \Rightarrow \{ \text{stepan}, \text{arkadyevitch}, \text{alexei}, \text{alexandrovitch} \} $ (0.927) \\
14 & $ \{ \text{alexandrovitch}, \text{ivanovna} \} \Rightarrow \{ \text{alexandrovitch}, \text{lidia}, \text{ivanovna} \} $ (0.926) \\
15 & $ \{ \text{arkadyevitch}, \text{alexei} \} \Rightarrow \{ \text{arkadyevitch}, \text{alexei}, \text{alexandrovitch} \} $ (0.925) \\

\end{tabulary}

\caption{Top 15 parallel association rules, by the weighted-window confidence.}
\end{table}

Table~\ref{table:tolstoy-rules} shows the top 15 association rules consisting of parallel episodes.

\begin{table}

\begin{tabulary}{\textwidth}{R|L}

\# & weighted-window confidence \\
\hline
1 & $ \text{sergei} \to \text{levin} \Rightarrow \text{sergei} \to \text{ivanovitch} \to \text{levin} $ (1) \\
2 & $ \text{alexandrovitch} \to \text{alexandrovitch} \Rightarrow \text{alexandrovitch} \to \text{alexei} \to \text{alexandrovitch} $ (1) \\
3 & $ \text{stepan} \to \text{levin} \Rightarrow \text{stepan} \to \text{arkadyevitch} \to \text{levin} $ (1) \\
4 & $ \text{stepan} \to \text{smile} \Rightarrow \text{stepan} \to \text{arkadyevitch} \to \text{smile} $ (1) \\
5 & $ \text{alexei} \to \text{alexandrovitch} \to \text{alexandrovitch} \Rightarrow \text{alexei} \to \text{alexandrovitch} \to \text{alexei} \to \text{alexandrovitch} $ (1) \\
6 & $ \text{stepan} \to \text{stepan} \Rightarrow \text{stepan} \to \text{arkadyevitch} \to \text{stepan} $ (1) \\
7 & $ \text{countess} \to \text{ivanovna} \Rightarrow \text{countess} \to \text{lidia} \to \text{ivanovna} $ (1) \\
8 & $ \text{levin} \to \text{arkadyevitch} \Rightarrow \text{levin} \to \text{stepan} \to \text{arkadyevitch} $ (1) \\
9 & $ \text{arkadyevitch} \to \text{arkadyevitch} \Rightarrow \text{arkadyevitch} \to \text{stepan} \to \text{arkadyevitch} $ (1) \\
10 & $ \text{alexei} \to \text{alexei} \to \text{alexandrovitch} \Rightarrow \text{alexei} \to \text{alexandrovitch} \to \text{alexei} \to \text{alexandrovitch} $ (0.984) \\
11 & $ \text{alexei} \to \text{alexei} \Rightarrow \text{alexei} \to \text{alexandrovitch} \to \text{alexei} $ (0.898) \\
12 & $ \text{alexandrovitch} \to \text{alexei} \to \text{alexandrovitch} \Rightarrow \text{alexei} \to \text{alexandrovitch} \to \text{alexei} \to \text{alexandrovitch} $ (0.846) \\
13 & $ \text{alexandrovitch} \to \text{alexandrovitch} \Rightarrow \text{alexei} \to \text{alexandrovitch} \to \text{alexei} \to \text{alexandrovitch} $ (0.846) \\
14 & $ \text{alexandrovitch} \to \text{alexandrovitch} \Rightarrow \text{alexei} \to \text{alexandrovitch} \to \text{alexandrovitch} $ (0.846) \\
15 & $ \text{alexei} \to \text{alexandrovitch} \to \text{alexei} \Rightarrow \text{alexei} \to \text{alexandrovitch} \to \text{alexei} \to \text{alexandrovitch} $ (0.833) \\

\end{tabulary}

\caption{Top 15 serial association rules, by the weighted-window confidence.}
\end{table}



\section{Comparison with non-frequency-based methods}

We'll carry out a comparative study with two implementations of mining algorithms that use interestingness measures not based on frequency.

Anti-monotonic freausncy measures inherently put larger episodes at a disadvantage, since an episode is never more frequent than any of its subepisodes. This has a few effects:
\begin{itemize}
\item $ (l + 1) $-episodes will generally score lower than $ l $-episodes, so larger episodes will tend to be ranked lower than smalles ones.
\item As a consequence of the preceding, and because we mine according to a fixed frequency threshold, the number of $ l $-episodes found decreases strongly as $ l $ grows.
\end{itemize}
We can clearly see the second effect if we plot the number of frequent episodes grouped by size: figure~\ref{fig:episode-frequencies-by-size} gives the numbers for one of the experiments performed in section~\ref{sec:performance-episodes}). We do see, however, that it helps to choose thresholds as low as possible, since the number greater-than-1-episodes rises significantly near the lowest thresholds, especially with parallel episodes and fixed-window frequency (figure~\ref{fig:episode-frequencies-by-size-parallel-fwi}), where at the lowest threshold, the number of 5-episodes exceeds the number of 1-episodes. Still, larger episodes generally won't score highly compared to their subepisodes, and so they won't do well when episodes are ranked.


We can make some more observations about the plots in figure~\ref{fig:episode-frequencies-by-size}. % oh yeah? we'll see about that

For serial episodes (figures~\ref{fig:episode-frequencies-by-size-serial-fwi}, \ref{fig:episode-frequencies-by-size-serial-mwi}, \ref{fig:episode-frequencies-by-size-serial-wwi}) the number of % WHAT??

\begin{figure}

\newcommand\nsfepisodefrequenciesbysizeaxis[1]{%
\begin{axis}[
    legend entries={1-episodes,2-episodes,3-episodes,4-episodes,5-episodes,6-episodes,7-episodes,8-episodes},
    legend style={legend pos=north east,legend style={nodes={scale=0.75}},fill opacity=0.7,text opacity=1},
    xlabel={frequency threshold},
    ylabel={number of frequent $ l $-episodes},
    xmode=log,
    ymode=log,
]

\addplot table [x=frequency-threshold,y=num-frequent-1-episodes] {#1};
\addplot table [x=frequency-threshold,y=num-frequent-2-episodes] {#1};
\addplot table [x=frequency-threshold,y=num-frequent-3-episodes] {#1};
\addplot table [x=frequency-threshold,y=num-frequent-4-episodes] {#1};
\addplot table [x=frequency-threshold,y=num-frequent-5-episodes] {#1};
\addplot table [x=frequency-threshold,y=num-frequent-6-episodes] {#1};
\addplot table [x=frequency-threshold,y=num-frequent-7-episodes] {#1};
\addplot table [x=frequency-threshold,y=num-frequent-8-episodes] {#1};

\end{axis}
}

\begin{subfigure}[b]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]

\nsfepisodefrequenciesbysizeaxis{experiments/nsf/nsf-parallel-fixed-windows-8.tsv}

\end{tikzpicture}
\caption{parallel, fixed windows}
\label{fig:episode-frequencies-by-size-parallel-fwi}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]
\nsfepisodefrequenciesbysizeaxis{experiments/nsf/nsf-serial-fixed-windows-8.tsv}

\end{tikzpicture}
\caption{serial, fixed windows}
\label{fig:episode-frequencies-by-size-serial-fwi}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]
\nsfepisodefrequenciesbysizeaxis{experiments/nsf/nsf-parallel-minimal-windows-8.tsv}

\end{tikzpicture}
\caption{parallel, minimal windows}
\label{fig:episode-frequencies-by-size-parallel-mwi}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]
\nsfepisodefrequenciesbysizeaxis{experiments/nsf/nsf-serial-minimal-windows-8.tsv}

\end{tikzpicture}
\caption{serial, minimal windows}
\label{fig:episode-frequencies-by-size-serial-mwi}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]
\nsfepisodefrequenciesbysizeaxis{experiments/nsf/nsf-parallel-weighted-windows-8.tsv}

\end{tikzpicture}
\caption{parallel, weighted windows}
\label{fig:episode-frequencies-by-size-serial-wwi}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]
\nsfepisodefrequenciesbysizeaxis{experiments/nsf/nsf-serial-weighted-windows-8.tsv}

\end{tikzpicture}
\caption{serial, weighted windows}
\label{fig:episode-frequencies-by-size-serial-wwi}
\end{subfigure}
\caption{A plot of the number of episodes found across different thresholds, episodes grouped by size. Episodes mined from dataset \emph{abstract} using the fixed-window frequency measure.}
\label{fig:episode-frequencies-by-size}
\end{figure}
\citep{cule2016efficient} introduced an interestingness measure called \emph{cohesion}. The cohesiveness of an episode expresses how closely the events of its occurrences are to each other, that is, the distance between events is taken into account.

An episode is deemed cohesive in regard to a sequence if the events that constitute an occurrence are close to each other; in other words, its minimal minimal windows are small on average.

Recall that the weighted-window frequency also takes into account the size of the minimal windows---it lends more weight to smaller windows. There is an important distinction, though:
\begin{itemize}
\item The weighted-window frequency assigns a weight to each minimal window (the inverse of its width) and sums the weights.
\item The cohesion is inversely proportional to the average width of the windows.
\end{itemize}

So, with the weighted-window frequency, a pattern that occurs often has an advantage over a less frequent pattern. With cohesion, that is not the case.

Additionally, the cohesion doesn't disadvantage larger patterns, as frequency measures inherently do due to their monotonic nature. The cohesion is proportional to the size of the pattern, which addresses the fact that larger patterns naturally have larger minimal windows.

While cohesiveness is not frequency-based, there is a frequency aspect, still: to combat the pattern explosion inherent to data mining, the event types that make up a cohesive episode should be frequent in the sequence, by a given threshold. Event types that don't have enough support won't appear in any pattern.

The \emph{cohesion} of an episode $ \alpha $ is defined as
\begin{align*}
C(\alpha) = \frac{| \alpha |}{\overline{W}(\alpha)}
\end{align*}
where $ \overline{W}(\alpha) $ is the average size of the minimal occurrences of $ \alpha $.

The mining algorithm \textsc{Fci} mines parallel episodes with an injective $ lab $-function, meaning that each event type appears at most once. The algorithm takes the following parameters:
\begin{itemize}
\item The minimal support that an event type must have in order to be considered at all.
\item The maximal size of patterns to generate.
\item The minimal cohesion that any pattern must have.
\end{itemize}
Further details can be found in~\cite{cule2016efficient}.

In~\cite{cule2016efficient}, the cohesion of a pattern is defined using the mean width of the minimal windows. The mean has a number of downsides, though; one of which is that the mean of a distribution is unstable if there are outliers. So for instance, if one window is significantly larger or smaller than the others, the cohesion may be greatly affected. Therefore, a quantile-based approach was proposed in~\citep{feremans2018mining}. Here, the minimal occurrences are grouped by width using a threshold. The quantile-based cohesion is then defined as the percentage of minimal occurrences that are smaller than the threshold. The threshold is proportional to the size of the episode, again in an effort not to disadvantage larger patterns.

Their mining method \textsc{Qcsp} mines serial episodes, and takes the following parameters:
\begin{itemize}
\item The minimal support that an event type must have in order to be considered at all.
\item The maximal size of patterns to generate.
\item The threshold mentioned above (which gets multiplied by the size of each episode being considered).
\item $ k $. The algorithm reports only the $ k $ patterns with the greatest cohesion.
\end{itemize}

For the quality experiments, we ran our implementation with a window width of 15, and with thresholds as low as possible within reasonable time. \textsc{Fci} was run with a minimum support of 5, a maximal pattern size of 5, and a minimal cohesion of 0.14 (a lower threshold started to take significantly more time).

\begin{table}
\centering

\begin{tabulary}{\textwidth}{R|L|L}

\# & cohesion & quantile-based cohesion \\
\hline
1 & $ \{ \text{agafea}, \text{mihalovna} \} $ (1) & $ \text{stepan} \to \text{arkadyevitch} $ (0.998, 1096) \\
2 & $ \{ \text{char}, \text{banc} \} $ (1) & $ \text{alexei} \to \text{alexandrovitch} $ (0.95, 1203) \\
3 & $ \{ \text{pinc}, \text{nez} \} $ (1) & $ \text{sergei} \to \text{ivanovitch} $ (0.965, 603) \\
4 & $ \{ \text{bell}, \text{soeur} \} $ (1) & $ \text{agafea} \to \text{mihalovna} $ (1, 148) \\
5 & $ \{ \text{stepan}, \text{arkadyevitch} \} $ (0.915) & $ \text{darya} \to \text{alexandrovna} $ (0.967, 424) \\
6 & $ \{ \text{nativ}, \text{tribe} \} $ (0.523) & $ \text{lidia} \to \text{ivanovna} $ (0.977, 221) \\
7 & $ \{ \text{lizaveta}, \text{petrovna} \} $ (0.408) & $ \text{lizaveta} \to \text{petrovna} $ (0.98, 49) \\
8 & $ \{ \text{ivanovna}, \text{lidia} \} $ (0.137) & $ \text{nativ} \to \text{tribe} $ (0.966, 29) \\
9 & $ \{ \text{alexandrovitch}, \text{alexei} \} $ (0.05) & $ \text{liza} \to \text{merkalova} $ (0.706, 34) \\
10 & $ \{ \text{sergei}, \text{ivanovitch} \} $ (0.0428) & $ \text{marya} \to \text{nikolaevna} $ (0.673, 98) \\
11 & $ \{ \text{darya}, \text{alexandrovna} \} $ (0.0363) & $ \text{countess} \to \text{lidia} \to \text{ivanovna} $ (0.586, 389) \\
12 & $ \{ \text{bezzubov}, \text{landau} \} $ (0.0351) & $ \text{vassili} \to \text{lukitch} $ (0.588, 51) \\
13 & $ \{ \text{gladiat}, \text{frou} \} $ (0.0326) & $ \text{countess} \to \text{lidia} $ (0.557, 280) \\
14 & $ \{ \text{partnership}, \text{ryezunov} \} $ (0.0303) & $ \text{countess} \to \text{ivanovna} $ (0.549, 277) \\
15 & $ \{ \text{bridal}, \text{lectern} \} $ (0.0281) & $ \text{madam} \to \text{stahl} $ (0.55, 171) \\

\end{tabulary}

\caption{The top 15 patterns mined from~\emph{tolstoy} using cohesion (\textsc{Fci}, minimum support 5, maximal size 5) and quantile-based cohesion (\textsc{Qcsp}, minimum support 7, maximal size 5).}
\label{table:cohesive-patterns}
\end{table}



Table~\ref{table:cohesive-patterns} shows the 15 most interesting episodes by the two differerent definitions of cohesion.
