\chapter{Experiments}
\label{sec:experiments}

In this chapter, we perform experiments to answer the research question we posed in the introduction. Concisely put, we want to know whether we can find patterns in long sequences in a reasonable time, and whether there are interesting patterns to be found.

After choosing a number of datasets and showing some of their basic characteristics, we assess the performance of the implementation by running it on datasets of different kinds, and varying the parameters.

First, we'll briefly discuss how we ensured the correctness of our implementation.

\section{Correctness}
\label{sec:experiments-correctness}

A big point of comparison while implementing our algorithms was a closed episode miner\footnote{http://adrem.ua.ac.be/mining-closed-strict-episodes}. It allowed us to verify the correctness of our implementation. The closed episode miner differs in a few ways from our implementation:

\begin{itemize}
\item It mines general episodes, not just parallel and serial episodes.
\item It mines closed episodes. An episode is closed if there are no superepisodes of the same frequency. Mining only closed episodes helps reduce the amount of output, by excluding non-closed episodes. The closed episode miner has options to mine non-closed episodes as well, which allows us to compare the output of both implementations.
\item While our implementation finds episodes of one class at a time, as specified by the user --- parallel or serial --- the closed episode miner finds episodes of all classes at once --- parallel, serial and general.
\end{itemize}

Given the above, in order to compare the two implementations in terms of their output, we have to enable the options that cause the closed episode miner to find non-closed episodes, since our implementation does not exclude non-closed episodes. With those options enabled, our implementation finds a subset of the episodes that the closed episode miner finds; and we have to filter out those episodes which don't match the class of episodes we're currently mining.

We validated the output of our implementation with the output of the closed episode miner, running the closed episode miner alongside our implementation on the same datasets and parameter configurations, then parsing and comparing the results. Thanks to the closed episode miner we were able to eliminate quite a few errors in our implementation.

We did find, however, a possible bug in the closed episode miner. When mining parallel episodes in the example sequence (Figure~\ref{fig:event-sequence}), using the weighted-window frequency and with a window width of 8, the frequencies for a few episodes differed, as shown in Table~\ref{table:closepi-frequency-difference}. Observing the sequence, each of these episodes has two overlapping minimal windows, of which the second one has a greater weight. Our implementation seems to correctly select the window with the higher weight, while the closed episode miner seems to choose the first window.

Further distilling the issue, we tested a simpler example: the sequence $ \langle (a, 1),\allowbreak(a, 3),\allowbreak(a, 4) \rangle $, episode $ \{ a, a \} $, and a window width of at least 3. The weighted-window frequency is clearly $ 1 / 2 $, being the weight of the minimal window $ [3, 5) $, but the closed episode miner reported $ 1 / 3 $, so it indisputably selected $ [1, 4) $.

\begin{table}
\centering

\begin{tabulary}{\textwidth}{ C|C|C }

$ \alpha $ & $ fr_w(\alpha) $ (\emph{ours}) & $ fr_w(\alpha) $ (\emph{Closepi}) \\
\hline
$ \{ b, d \} $ & $ 0.2 $ ($ 1/5 $) & $ 0.166 \ldots $ ($ 1/6 $) \\
$ \{ a, b, d \} $ & $ 0.2 $ ($ 1/5 $) & $ 0.142857 \ldots $ ($ 1/7 $) \\
$ \{ a, b, e \} $ & $ 0.25 $ ($ 1/4 $) & $ 1.66 \ldots $ ($ 1/6 $) \\

\end{tabulary}

\caption{Differing weighted-window frequency values between our implementation (\emph{ours}) and the closed episode miner (\emph{Closepi}), mining the example sequence from Figure~\ref{fig:event-sequence}.}
\label{table:closepi-frequency-difference}
\end{table}

\section{Datasets}

We will conduct experiments using a number of datasets:

\begin{itemize}
\item \emph{abstract}: a dataset consisting of the first 739 NSF award abstracts from 1990, merged into one long sequence\footnote{\url{http://kdd.ics.uci.edu/databases/nsfabs/nsfawards.html}}.
\item \emph{tolstoy}: Leo Tolstoy's novel Anna Karenina, part of the public domain, and available at Project Gutenberg\footnote{\url{https://www.gutenberg.org/ebooks/1399}}.
\item \emph{trains}: a dataset consisting of departure times of delayed trains in a Belgian railway station, for trains with a delay of at least three minutes. This data is anonymized, so we won't be able derive any meaning from the patterns, but it it an interesting dataset nonetheless, because contrary to the textual datasets, \emph{trains} contains sparse, real-time data. The time interval between two subsequent timestamps represents one second. Its timespan of over 2.66~million seconds converts to over 730~hours.
\end{itemize}

The textual datasets were preprocessed by lemmatizing using the Porter stemmer\footnote{\url{https://tartarus.org/martin/PorterStemmer}} and by removing stop words.

Table~\ref{table:datasets-numbers} shows some statistics about each dataset, where $ | \Sigma | $ is the size of the alphabet, $ | s | $ is the number of events, and $ T_e - T_s $ is the time range of the sequence. For dense sequences, $ T_e - T_s = | s | $. For sparse sequences, a window of $ \rho $ contains on average $ | s | \frac\rho{T_e - T_s} $ events. For instance, a window of 900 in \emph{trains} (15 minutes) contains approximately 3.42 events on average.

We see that all of the datasets have quite large alphabets. Figure~\ref{fig:alphabet-frequencies} shows the number of occurrences of the most frequent event types, ordered by frequency, for each of the datasets. We observe a long tail for \emph{abstract} and \emph{tolstoy}, with a small number of the event types occurring a significant number of times, and the vast majority of event types occurring very rarely. This is common for natural-language texts. For \emph{trains} the event types aren't uniformly distributed either, but the tail is not very long. Note that the graphs for the textual datasets don't even show all event types, although for \emph{abstract} the rightmost event types occur only once, and those for \emph{tolstoy} occur fewer than 10~times.

Figure~\ref{fig:trains-cumulative} shows the cumulative distribution of event types in the \emph{trains} dataset. We see that some periods of time are noticeably more active than others, so some windows will contain more information than others. With some imagination, in Figure~\ref{fig:trains-cumulative-part} one might be able to discern rush hour from quieter periods, and day from night --- perhaps even weekdays from the weekend.

\begin{table}[t]
\centering

\begin{tabulary}{\textwidth}{ L|RRRC }

dataset & \multicolumn{1}{c}{$ | \Sigma | $} & \multicolumn{1}{c}{$ | s | $} & \multicolumn{1}{c}{$ T_e - T_s $} & type \\
\hline
\emph{abstract} & $ 51\,346 $ & $ 67\,828 $ & $ 67\,828 $ & dense \\
\emph{tolstoy} & $ 95\,623 $ & $ 124\,627 $ & $ 124\,627 $ & dense \\
\emph{trains} & $ 1\,280 $ & $ 10\,115 $ & $ 2\,662\,667 $ & sparse \\

\end{tabulary}

\caption{Some properties of the datasets $ (s, T_s, T_e) $.}
\label{table:datasets-numbers}
\end{table}
\begin{figure}[t]

\begin{subfigure}[b]{0.48\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    xlabel={event types ordered by frequency},
    ylabel={number of occurrences},
    xmin=-10,
    ymin=-50,
    % ymode=log
]

\addplot table [x=rank,y=count,mark=none] {experiments/nsf-alphabet-frequency-300.dat};

\end{axis}

\end{tikzpicture}

\caption{The 300 most frequent event types in \emph{abstract}.}
\label{fig:frequency-plot-abstract}
\end{subfigure}\hfill%
\begin{subfigure}[b]{0.48\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    xlabel={event types ordered by frequency},
    ylabel={number of occurrences},
    xmin=-100,
    ymin=-50,
    % ymode=log
]

\addplot table [x=rank,y=count,mark=none] {experiments/tolstoy-alphabet-frequency-2200.dat};

\end{axis}

\end{tikzpicture}

\caption{The 2200 most frequent event types in \emph{tolstoy}.}
\label{fig:frequency-plot-tolstoy}
\end{subfigure}

\par\bigskip

\begin{subfigure}[b]{\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    xlabel={event types ordered by frequency},
    ylabel={number of occurrences},
    xmin=-50,
    ymin=-1,
    % ymode=log
]

\addplot table [x=rank,y=count,mark=none] {experiments/trains-alphabet-frequency.tsv};

\end{axis}

\end{tikzpicture}

\caption{The event types in \emph{trains}.}
\label{fig:frequency-plot-trains}
\end{subfigure}

\caption{The frequency of event types in the datasets.}
\label{fig:alphabet-frequencies}
\end{figure}

\begin{figure}
\begin{subfigure}{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    xlabel={timestamp},
    ylabel={cumulative events},
    % xmin=-10,
    % ymin=-50,
    % ymode=log
]

\addplot table [x=time,y=count,mark=none] {experiments/trains-cumulative.dat};

\end{axis}

\end{tikzpicture}
\caption{The whole sequence.}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    xlabel={timestamp},
    ylabel={cumulative events},
    % xmin=-10,
    % ymin=-50,
    % ymode=log
]

\addplot table [x=time,y=count,mark=none] {experiments/trains-cumulative-part.dat};

\end{axis}

\end{tikzpicture}
\caption{A period of one week.}
\label{fig:trains-cumulative-part}
\end{subfigure}
\caption{The cumulative distribution of events in the sparse event sequence \emph{trains}.}
\label{fig:trains-cumulative}
\end{figure}


\section{Performance}
\label{sec:performance}

To assess the efficiency of the algorithm, we inspect the runtime for different input parameters: episode classes and frequency measures, over a range of window widths and thresholds.

The performance experiments were ran as follows. For specified episode classes, frequency measures, a list of window widths, and a range of frequency thresholds, an experiment would run the Cartesian product of all these parameters, within time and memory constraints.

For each configuration of episode class, frequency measure, and window width, the frequency threshold is lowered exponentially, using a multiplier $ \in (0, 1) $. For example, with a multiplier of 0.9, the next threshold is always 10\% smaller than the last. If memory runs out or the timeout is exceeded before reaching the lower bound, all lower frequency thresholds for that configuration are skipped, as they will take at least as much time and memory as the current threshold.

All performance experiments were run on the same machine; the full specifications of which can be found online\footnote{The specifications can be found at \url{https://support.apple.com/kb/sp623}. 2.7 GHz model; memory manually upgraded to 12~GB. Running macOS 10.13.5. All C++ code compiled with clang-900.0.39.2 from LLVM~9.0. All Java code run with Java~SE~1.8.}.

% TODO maybe use window width 15 (re-run experiment with different multiplier probably)

\subsection{Episodes}
\label{sec:performance-episodes}

\begin{figure}

\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=south east},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
    xmode=log,
    ymode=log,
]

\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-parallel-fixed-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-parallel-minimal-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-parallel-weighted-windows-8.tsv};

\end{axis}

\end{tikzpicture}

\caption{parallel episodes}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=south east},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
    xmode=log,
    ymode=log,
]

\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-serial-fixed-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-serial-minimal-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-serial-weighted-windows-8.tsv};

\end{axis}

\end{tikzpicture}

\caption{serial episodes}
\end{subfigure}

\caption{Runtimes for finding episodes in dataset \emph{abstract} using a window width of 8.}
\label{fig:runtimes-nsf-8}
\end{figure}

\begin{figure}

\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=south east},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
    xmode=log,
    ymode=log,
]

\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/tolstoy/tolstoy-parallel-fixed-windows-15.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/tolstoy/tolstoy-parallel-minimal-windows-15.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/tolstoy/tolstoy-parallel-weighted-windows-15.tsv};

\end{axis}

\end{tikzpicture}

\caption{parallel episodes}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=north west},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
    xmode=log,
    ymode=log,
]

\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/tolstoy/tolstoy-serial-fixed-windows-15.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/tolstoy/tolstoy-serial-minimal-windows-15.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/tolstoy/tolstoy-serial-weighted-windows-15.tsv};

\end{axis}

\end{tikzpicture}

\caption{serial episodes}
\end{subfigure}

\caption{Runtimes for finding episodes in dataset \emph{tolstoy} using a window width of 15.}
\label{fig:runtimes-tolstoy-15}
\end{figure}

\begin{figure}

\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=south east},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
    xmode=log,
    ymode=log,
]

\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/trains/trains-parallel-minimal-windows-900.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/trains/trains-parallel-weighted-windows-900.tsv};

\end{axis}

\end{tikzpicture}

\caption{parallel episodes}
\label{fig:runtimes-trains-900-parallel}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=south east},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
    xmode=log,
    ymode=log,
]

\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/trains/trains-serial-fixed-windows-900.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/trains/trains-serial-minimal-windows-900.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/trains/trains-serial-weighted-windows-900.tsv};

\end{axis}

\end{tikzpicture}

\caption{serial episodes}
\label{fig:runtimes-trains-900-serial}
\end{subfigure}

\caption{Runtimes for finding episodes in dataset \emph{trains} using a window width of 900.}
\label{fig:runtimes-trains-900}
\end{figure}

We would like to compare the efficiency for the different frequency measures across a range of frequency thresholds. However we should not compare the runtimes of the frequency measures based on the frequency thresholds directly, since the values for each of the measures are semantically different. Instead we can compare the runtimes as a function of the number of frequent episodes produced.

The graph in Figure~\ref{fig:runtimes-nsf-8} shows runtimes for mining episodes from \emph{abstract}. We see that, for parallel episodes, the fixed-window frequency takes significantly less time to generate a given amount of episodes than the other measures. The disjoint-window frequency and the weighted-window frequency are very close to each other until approximately one thousand episodes are found; then they start to diverge, and the the weighted-window frequency takes more time than the other two.

% TODO v check if still valid with new experiments (window size 15)

For serial episodes, the fixed-window frequency has much less of an advantage. In fact, the disjoint-window frequency seems to overtake the fixed-window frequency for large amounts of episodes produced. This could be explained by the fact that the data pass algorithm that finds minimal windows of serial episodes is slightly simpler than the algorithm that determines the fixed-window frequency of serial episodes. Again the weighted-window frequency is slower, but the difference is smaller this time. Also note that for all measures, runtimes for serial episodes were generally higher --- since the experiments were time-constrained, the graph stops around the order of $ 10^5 $ for parallel episodes, and around $ 10^3 $ for serial episodes.

Figure~\ref{fig:runtimes-tolstoy-15} shows the results for \emph{tolstoy}. Though \emph{tolstoy} is approximately twice as long as \emph{abstract}, the progressions are very similar. Given these results, we think that it is likely that most texts with a typical frequency distribution of words (as in Figure~\ref{fig:frequency-plot-abstract} and Figure~\ref{fig:frequency-plot-tolstoy}) will show a similar progression.

The results for the sparse dataset \emph{trains} are a bit different (Figure~\ref{fig:runtimes-trains-900}). For parallel episodes, the fixed-window frequency is faster again.

Curiously, the time consumption for the weighted-window frequency increases steadily, until around $ 10^3 $ episodes or so, when it plateaus, and eventually matches the disjoint-window frequency. For serial episodes, the weighted-window frequency shows a similar plateau.

Figure~\ref{fig:investigate-trains-runtimes} provides some insight. If we plot the runtime as a function of the frequency threshold (Figure~\ref{fig:investigate-trains-runtimes-runtimes}), we see that the runtime plateaus around a frequency threshold of 1. On the right hand plot (Figure~\ref{fig:investigate-trains-runtimes-frequencies}), we see that for thresholds less than 1, the number of 1-episodes doesn't increase. That's because all possible 1-episodes (the entire alphabet) are now frequent. This means that the number of 2-candidates does not increase anymore. Further lowering the threshold, more of those 2-episodes start becoming frequent at no extra cost. Only after a number of 2-episodes starts becoming frequent, the runtime starts increasing significantly again; when a significant amount of greater-than-2-episodes starts becoming frequent.

For the other frequency measures, we don't see such a kink in the graph, and that's because those never reached the point where the full alphabet becomes frequent, since they are more forgiving towards larger episodes than the weighted-window frequency.

\begin{figure}
\begin{subfigure}[t]{0.48\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={parallel episodes,serial episodes},
    legend style={legend pos=south west},
    xlabel={frequency threshold},
    ylabel={runtime (s)},
    xmode=log,
    ymode=log,
    % x dir=reverse,
]

\addplot table [x=frequency-threshold,y=duration-s] {experiments/trains/trains-parallel-weighted-windows-900.tsv};
\addplot table [x=frequency-threshold,y=duration-s] {experiments/trains/trains-serial-weighted-windows-900.tsv};

\end{axis}

\end{tikzpicture}
\caption{Runtime as a function of the frequency threshold.}
\label{fig:investigate-trains-runtimes-runtimes}
\end{subfigure}%
\hfill%
\begin{subfigure}[t]{0.48\textwidth}
\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={1-episodes,2-episodes,3-episodes,4-episodes,5-episodes,6-episodes,7-episodes,8-episodes,9-episodes,10-episodes,11-episodes,12-episodes,13-episodes,14-episodes,15-episodes},
    legend style={legend pos=north east,legend style={nodes={scale=0.75}}},
    xlabel={frequency threshold},
    ylabel={number of frequent $ l $-episodes},
    xmode=log,
    ymode=log,
]

\addplot table [x=frequency-threshold,y=num-frequent-1-episodes] {experiments/trains/trains-parallel-weighted-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-2-episodes] {experiments/trains/trains-parallel-weighted-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-3-episodes] {experiments/trains/trains-parallel-weighted-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-4-episodes] {experiments/trains/trains-parallel-weighted-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-5-episodes] {experiments/trains/trains-parallel-weighted-windows-900.tsv};

\end{axis}
\end{tikzpicture}
\caption{Number of frequent $ l $-episodes as a function of the frequency threshold. Not all episode sizes shown, and only parallel episodes.}
\label{fig:investigate-trains-runtimes-frequencies}
\end{subfigure}
\caption{Some more plots as a subject of investigation for the curious runtimes for the weighted windows in experiment in Figure~\ref{fig:runtimes-trains-900}.}
\label{fig:investigate-trains-runtimes}
\end{figure}

Note that significantly more episodes are found in the \emph{trains} dataset --- up to the order of $ 10^6 $. This is most likely due to the fact that \emph{trains} is considerably smaller than \emph{abstract} and \emph{tolstoy}, in both alphabet size and number of events. Perhaps, if given more time, the progression would look similar for \emph{abstract} and \emph{tolstoy}.

\begin{figure}

\newcommand\nsfepisodefrequenciesbysizeaxis[1]{%
\begin{axis}[
    legend entries={1-episodes,2-episodes,3-episodes,4-episodes,5-episodes,6-episodes,7-episodes,8-episodes},
    legend style={legend pos=north east,legend style={nodes={scale=0.75}}},
    xlabel={frequency threshold},
    ylabel={number of frequent $ l $-episodes},
    xmode=log,
    ymode=log,
]

\addplot table [x=frequency-threshold,y=num-frequent-1-episodes] {#1};
\addplot table [x=frequency-threshold,y=num-frequent-2-episodes] {#1};
\addplot table [x=frequency-threshold,y=num-frequent-3-episodes] {#1};
\addplot table [x=frequency-threshold,y=num-frequent-4-episodes] {#1};
\addplot table [x=frequency-threshold,y=num-frequent-5-episodes] {#1};
\addplot table [x=frequency-threshold,y=num-frequent-6-episodes] {#1};
\addplot table [x=frequency-threshold,y=num-frequent-7-episodes] {#1};
\addplot table [x=frequency-threshold,y=num-frequent-8-episodes] {#1};

\end{axis}
}

\begin{subfigure}[b]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]

\nsfepisodefrequenciesbysizeaxis{experiments/nsf/nsf-parallel-fixed-windows-8.tsv}

\end{tikzpicture}
\caption{\emph{abstract}}
\label{fig:episode-frequencies-by-size-abstract}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={1-episodes,2-episodes,3-episodes,4-episodes,5-episodes,6-episodes,7-episodes,8-episodes,9-episodes,10-episodes,11-episodes,12-episodes,13-episodes,14-episodes,15-episodes},
    legend style={legend pos=outer north east,legend style={nodes={scale=0.75}}},
    xlabel={frequency threshold},
    ylabel={number of frequent $ l $-episodes},
    xmode=log,
    ymode=log,
]

\addplot table [x=frequency-threshold,y=num-frequent-1-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-2-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-3-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-4-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-5-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-6-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-7-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-8-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-9-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-10-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-11-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-12-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-13-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-14-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=frequency-threshold,y=num-frequent-15-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};


\end{axis}

\end{tikzpicture}
\caption{\emph{trains}}
\label{fig:episode-frequencies-by-size-trains}
\end{subfigure}
\caption{Number of parallel episodes found in \emph{abstract} and \emph{trains} for the fixed-window frequency across different thresholds, episodes grouped by size.}
\label{fig:episode-frequencies-by-size}
\end{figure}

Besides runtime comparisons, it can also be useful to know how many episodes of each size are produced. In Figure~\ref{fig:episode-frequencies-by-size} we see that for low thresholds lots of larger episodes become frequent. For \emph{abstract} we find up to 8-episodes and for \emph{trains} we even find up to 15-episodes.

Plots for the other kinds of episodes and frequency measures showed similar results, though there were fewer large episodes. Conceivably that can be explained by the fact that all experiments were run under the same time constraints, and as we observed previously, the implementation that mines parallel episodes by the fixed-window frequency usually produces larger amounts of episodes within a given amount of time.

% TODO maybe write some more about these cool graphs maybe


\subsection{Association rules}
\label{sec:performance-rules}

\begin{figure}

\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=north west},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
    xmode=log,
    ymode=log,
]

\addplot table [x=num-frequent-episodes,y=duration-rules-0.0] {experiments/tolstoy/tolstoy-parallel-fixed-windows-15.tsv};
\addplot table [x=num-frequent-episodes,y=duration-rules-0.0] {experiments/tolstoy/tolstoy-parallel-minimal-windows-15.tsv};
\addplot table [x=num-frequent-episodes,y=duration-rules-0.0] {experiments/tolstoy/tolstoy-parallel-weighted-windows-15.tsv};

\end{axis}

\end{tikzpicture}

\caption{rules consisting of parallel episodes}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=north west},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
    xmode=log,
    ymode=log,
]

\addplot table [x=num-frequent-episodes,y=duration-rules-0.0] {experiments/tolstoy/tolstoy-serial-fixed-windows-15.tsv};
\addplot table [x=num-frequent-episodes,y=duration-rules-0.0] {experiments/tolstoy/tolstoy-serial-minimal-windows-15.tsv};
\addplot table [x=num-frequent-episodes,y=duration-rules-0.0] {experiments/tolstoy/tolstoy-serial-weighted-windows-15.tsv};

\end{axis}

\end{tikzpicture}

\caption{rules consisting of serial episodes}
\end{subfigure}

\caption{Runtimes for finding association rules using episodes generated from \emph{tolstoy} as a function of the number of frequent episodes.}
\label{fig:runtimes-rules-tolstoy-15}
\end{figure}

\begin{figure}

\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=north west},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
    xmode=log,
    ymode=log,
]

\addplot table [x=num-frequent-episodes,y=duration-rules-0.9] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=num-frequent-episodes,y=duration-rules-0.9] {experiments/trains/trains-parallel-minimal-windows-900.tsv};
\addplot table [x=num-frequent-episodes,y=duration-rules-0.9] {experiments/trains/trains-parallel-weighted-windows-900.tsv};

\end{axis}

\end{tikzpicture}

\caption{rules consisting of parallel episodes}
\label{fig:runtimes-rules-trains-900-parallel}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=north west},
    xlabel={number of frequent episodes},
    ylabel={runtime (s)},
    xmode=log,
    ymode=log,
]

\addplot table [x=num-frequent-episodes,y=duration-rules-0.9] {experiments/trains/trains-serial-fixed-windows-900.tsv};
\addplot table [x=num-frequent-episodes,y=duration-rules-0.9] {experiments/trains/trains-serial-minimal-windows-900.tsv};
\addplot table [x=num-frequent-episodes,y=duration-rules-0.9] {experiments/trains/trains-serial-weighted-windows-900.tsv};

\end{axis}

\end{tikzpicture}

\caption{rules consisting of serial episodes}
\label{fig:runtimes-rules-trains-900-serial}
\end{subfigure}

\caption{Runtimes for finding association rules using episodes generated from \emph{trains} as a function of the number of frequent episodes.}
\label{fig:runtimes-rules-trains-900}
\end{figure}

Besides episodes, we would also like to have a sense of the runtimes for generating association rules. All of the results shown in this section were generated during the experiments from Section~\ref{sec:performance-episodes}, so the same parameter configurations apply.

For a given set of episodes, the choice of the confidence threshold mostly depends on space requirements --- time less so. That is because the number of association rules that needs to be considered does not depend on the chosen threshold, and the only time cost for choosing a low confidence threshold is in the fact that more episodes will need to be stored. The space cost, however, can increase dramatically. Experimentally, we found that it was feasible to store all association rules generated from the episodes found frequent for \emph{tolstoy}, but not those for \emph{trains}. So, for \emph{tolstoy} we ran with a confidence threshold of 0 --- further filtering could be applied in post-processing if desired --- and for \emph{trains} a threshold of 0.9 turned out to be reasonable in terms of storage.

As we saw in Algorithm~\ref{alg:association-rules-top-level}, for each frequent episode $ \beta $, all association rules $ \alpha \Rightarrow \beta $ with $ \alpha \subset \beta $ are considered. Assuming $ \beta $ has an injective $ lab $-function (an event type appears at most once), the number of subepisodes of an episode is exponential in the size of the episode; and so the same holds for the number of association rules to be considered.

Figure~\ref{fig:runtimes-rules-tolstoy-15} shows runtimes for generating association rules from \emph{tolstoy} using different episode classes, as a function of the number of frequent episodes. Despite the concerns we expressed in the previous paragraph, the runtime seems to be polynomial --- indicating that the number of frequent episodes remains the most important factor for these experiments.

Finding the association rules from parallel episodes in \emph{trains} using the fixed-window frequency (Figure~\ref{fig:runtimes-rules-trains-900-parallel}) started to take significant amounts of time for the lowest frequency thresholds (largest quantities of episodes produced). However, given that the graphs in the log--log plot are mostly linear, we would judge the complexity to be polynomial.

On the contrary, for association rules from serial episodes in \emph{trains}, the graph does curve upwards somewhat towards the end (Figure~\ref{fig:runtimes-rules-trains-900-parallel}), possibly due to the exponential relation described earlier. From the experiments in the previous section the amounts of larger serial episodes seemed to rise more dramatically than those for parallel episodes (Figure~\ref{fig:episode-frequencies-by-size-trains}).

% TODO weren't able to account for the difference between parallel and serial in this case; a possible explanation is that the amounts of larger episodes increased more drasticaly for serial episodes ...

We mentioned earlier that computing the fixed-window confidence is trivial, given the frequency of the episodes; and yet for some of the experiments here, finding the confident association rules took longer than the weighted-window confidence. This can be explained by the overhead of storing the rules that turned out to be confident, requiring more time if there are more confident rules.

\iffalse
\subsection{Number of candidate episodes versus number of frequent episodes}

% --- Plotting the number of candidates and the number of frequent episodes. Maybe not so useful.
\begin{tikzpicture}

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=outer north east},
    xlabel={number of candidates},
    ylabel={number of frequent episodes},
]

\addplot table [x=num-frequent-episodes,y=num-candidates] {experiments/nsf/nsf-parallel-fixed-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=num-candidates] {experiments/nsf/nsf-parallel-minimal-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=num-candidates] {experiments/nsf/nsf-parallel-weighted-windows-8.tsv};

\end{axis}

\end{tikzpicture}

\begin{tikzpicture}

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=outer north east},
    xlabel={number of candidates},
    ylabel={number of frequent episodes},
]

\addplot table [x=num-frequent-episodes,y=percentage-frequent-of-candidates] {experiments/nsf/nsf-parallel-fixed-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=percentage-frequent-of-candidates] {experiments/nsf/nsf-parallel-minimal-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=percentage-frequent-of-candidates] {experiments/nsf/nsf-parallel-weighted-windows-8.tsv};

\end{axis}

\end{tikzpicture}

\begin{tikzpicture}

\begin{axis}[
    legend entries={fixed windows,minimal windows,weighted windows},
    legend style={legend pos=outer north east},
    xlabel={number of candidates},
    ylabel={number of frequent episodes},
]

\addplot table [x=num-candidates,y=num-frequent-episodes] {experiments/trains/trains-parallel-fixed-windows-900.tsv};
\addplot table [x=num-candidates,y=num-frequent-episodes] {experiments/trains/trains-parallel-minimal-windows-900.tsv};
\addplot table [x=num-candidates,y=num-frequent-episodes] {experiments/trains/trains-parallel-weighted-windows-900.tsv};

\end{axis}

\end{tikzpicture}
% --- ---

The database passes are an expensive --- if not the most expensive --- part of the mining algorithm. Plotting the number of frequent episodes in the output and the number of candidates that were considered in a pass over the sequence gives us an idea of the amount of internal work the algorithms deal with for the output that gets produced.
\fi


\subsection{Window width}

The window width places an upper bound on the maximal distance between two events that are part of an occurrence of an episode. Consequently this also limits the number of candidate patterns. We know that the number of candidate episodes is exponential in the number of nodes. So we would like to have a sense of how the window width affects the runtimes.

For each combination of episode class and frequency measure, we decided on a threshold for which the algorithm finishes in a reasonable amount of time for a given window width. We used a window width 15 for \emph{abstract} and 900 for \emph{trains}. Then we varied the window width --- 3, 6, 9, 12, 15 for \emph{abstract} and 180, 360, 540, 720, 900 for \emph{trains}. Because we selected the thresholds in this way, the absolute values are of no importance; only the progression is.

The results for \emph{abstract} are shown in Figure~\ref{fig:abstract-window-width}. For parallel episodes, the fixed-window frequency seems to be affected the most. The weighted-window frequency doesn't seem to be affected much at all. This is likely due to the fact that it naturally produces fewer large episodes, given its preference for small minimal windows. As we mentioned before, in Section~\ref{sec:weighted-window-frequency}, weighted-windows frequency values stabilize as the window width grows.

Figure~\ref{fig:trains-window-width} shows the results for \emph{trains}. Interestingly, the runtimes for parallel episodes and fixed-window frequency remain quite stable until a window width of 720, after which it jumps upwards significantly. The number of frequent episodes grew over eight times as large. The particular characteristics of the \emph{trains} dataset may contribute, although we have no plausible explanation at the moment. In the same interval, the runtimes for the disjoint-window frequency increased quite a bit, whereas it progressed in a more stable manner in the other graphs.

The general tendency for the fixed-window frequency to be more affected than the other measures might be partly attributable to the amplification effect described in Section~\ref{sec:fixed-window-frequency}.

While these results give an indication of the influence of the window width on the runtime, they are not conclusive, because since the experiments are time-consuming, they don't cover very large window widths, nor very low thresholds.

For reference, Table~\ref{table:window-width-parameters} shows the frequency thresholds that were used.

\begin{figure}
\begin{subfigure}{0.48\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend pos=north west,
    legend entries={fixed windows,minimal windows,weighted windows},
    xlabel={window width},
    ylabel={runtime (s)},
    ymax=90,
    ymin=0,
    % xmode=log,
    % ymode=log,
]

\addplot table [x=window-width,y=duration-s] {experiments/nsf-width/nsf-width-parallel-fixed-windows-parallel-fixed-windows.tsv};
\addplot table [x=window-width,y=duration-s] {experiments/nsf-width/nsf-width-parallel-minimal-windows-parallel-minimal-windows.tsv};
\addplot table [x=window-width,y=duration-s] {experiments/nsf-width/nsf-width-parallel-weighted-windows-parallel-weighted-windows.tsv};

\end{axis}

\end{tikzpicture}
\caption{parallel episodes}
\end{subfigure}\hfill%
\begin{subfigure}{0.48\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend pos=south east,
    legend entries={fixed windows,minimal windows,weighted windows},
    xlabel={window width},
    ylabel={runtime (s)},
    ymin=0,
    % xmode=log,
    % ymode=log,
]

\addplot table [x=window-width,y=duration-s] {experiments/nsf-width/nsf-width-serial-fixed-windows-serial-fixed-windows.tsv};
\addplot table [x=window-width,y=duration-s] {experiments/nsf-width/nsf-width-serial-minimal-windows-serial-minimal-windows.tsv};
\addplot table [x=window-width,y=duration-s] {experiments/nsf-width/nsf-width-serial-weighted-windows-serial-weighted-windows.tsv};

\end{axis}

\end{tikzpicture}
\caption{serial episodes}
\end{subfigure}
\caption{Runtimes for mining episodes from \emph{abstract}, varying the window width.}
\label{fig:abstract-window-width}
\end{figure}

\begin{figure}
\begin{subfigure}{0.48\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend pos=north west,
    legend entries={fixed windows,minimal windows,weighted windows},
    xlabel={window width},
    ylabel={runtime (s)},
    ymin=0,
    % xmode=log,
    % ymode=log,
]

\addplot table [x=window-width,y=duration-s] {experiments/trains-width/trains-width-parallel-fixed-windows-parallel-fixed-windows.tsv};
\addplot table [x=window-width,y=duration-s] {experiments/trains-width/trains-width-parallel-minimal-windows-parallel-minimal-windows.tsv};
\addplot table [x=window-width,y=duration-s] {experiments/trains-width/trains-width-parallel-weighted-windows-parallel-weighted-windows.tsv};

\end{axis}

\end{tikzpicture}
\caption{parallel episodes}
\end{subfigure}\hfill%
\begin{subfigure}{0.48\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend pos=north west,
    legend entries={fixed windows,minimal windows,weighted windows},
    xlabel={window width},
    ylabel={runtime (s)},
    ymin=0,
    % xmode=log,
    % ymode=log,
]

\addplot table [x=window-width,y=duration-s] {experiments/trains-width/trains-width-serial-fixed-windows-serial-fixed-windows.tsv};
\addplot table [x=window-width,y=duration-s] {experiments/trains-width/trains-width-serial-minimal-windows-serial-minimal-windows.tsv};
\addplot table [x=window-width,y=duration-s] {experiments/trains-width/trains-width-serial-weighted-windows-serial-weighted-windows.tsv};

\end{axis}

\end{tikzpicture}
\caption{serial episodes}
\end{subfigure}
\caption{Runtimes for mining episodes from \emph{trains}, varying the window width.}
\label{fig:trains-window-width}
\end{figure}

\begin{table}
\begin{subtable}{\textwidth}
\centering
\begin{tabulary}{\textwidth}{L|L|R}
episode class & frequency measure & frequency threshold \\
\hline
parallel & fixed-window frequency & 29 \\
parallel & disjoint-window frequency & 23 \\
parallel & weighted-window frequency & 4.11 \\
serial & fixed-window frequency & 246 \\
serial & disjoint-window frequency & 23 \\
serial & weighted-window frequency & 14.5 \\
\end{tabulary}
\caption{\emph{abstract}}
\end{subtable}

\begin{subtable}{\textwidth}
\centering
\begin{tabulary}{\textwidth}{L|L|R}
episode class & frequency measure & frequency threshold \\
\hline
parallel & fixed-window frequency & 843 \\
parallel & disjoint-window frequency & 9 \\
parallel & weighted-window frequency & 0.0113 \\
serial & fixed-window frequency & 632 \\
serial & disjoint-window frequency & 3 \\
serial & weighted-window frequency & 0.00358 \\
\end{tabulary}
\caption{\emph{trains}}
\end{subtable}
\caption{The frequency thresholds used for the results of Figure~\ref{fig:abstract-window-width} and Figure~\ref{fig:trains-window-width}}.
\label{table:window-width-parameters}
\end{table}

\subsection{Sequence length}

In sequential pattern mining, sequences are expected to be very long. Therefore we would like to know how the length of the sequence affects the performance. We measured the runtime for mining episodes from different-sized prefixes of the \emph{tolstoy} dataset for all episode classes and frequency measures, using a window width of 15. The results for parallel episodes can be seen in Figure~\ref{fig:tolstoy-runtime-vs-length}. The results for serial episodes were similar.

As in the previous section, frequency thresholds were chosen based on the runtime. So again, when studying the graphs, only the progressions are important.

The general progression is fairly similar for all of the measures, although the minimal-windows-based measures rise quicker than the fixed-window frequency. While unfortunately the complexity is worse than linear (Figure~\ref{fig:tolstoy-runtime-vs-length-linear-scale}), judging by the mostly straight lines in the log--log plot (Figure~\ref{fig:tolstoy-runtime-vs-length-log-scale}) it does seem to be polynomial.

For reference, Table~\ref{table:tolstoy-runtime-vs-length-thresholds} shows the frequency thresholds that were used.

\begin{figure}

\begin{subfigure}[b]{0.5\textwidth}
\centering

\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend pos=north west,
    legend entries={fixed windows,minimal windows,weighted windows},
    xlabel={portion of the whole sequence},
    ylabel={runtime (s)},
    ymin=0,
    % ymode=log,
]

\addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-parallel-fixed-windows.dat};
\addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-parallel-minimal-windows.dat};
\addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-parallel-weighted-windows.dat};

\end{axis}

\end{tikzpicture}
\caption{linear scale}
\label{fig:tolstoy-runtime-vs-length-linear-scale}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.65]

\begin{axis}[
    legend pos=south east,
    legend entries={fixed windows,minimal windows,weighted windows},
    xlabel={portion of the whole sequence},
    ylabel={runtime (s)},
    xmode=log,
    ymode=log,
]

\addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-parallel-fixed-windows.dat};
\addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-parallel-minimal-windows.dat};
\addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-parallel-weighted-windows.dat};

% \addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-serial-fixed-windows.dat};
% \addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-serial-minimal-windows.dat};
% \addplot table [x=portion,y=duration-s] {experiments/length/tolstoy-lengths-serial-weighted-windows.dat};

\end{axis}

\end{tikzpicture}
\caption{log--log scale}
\label{fig:tolstoy-runtime-vs-length-log-scale}
\end{subfigure}

\caption{Runtimes for different portions of \emph{tolstoy} using a fixed frequency threshold for each frequency measure.}
\label{fig:tolstoy-runtime-vs-length}
\end{figure}

\begin{table}
\centering
\begin{tabulary}{\textwidth}{L|L|R}
episode class & frequency measure & frequency threshold \\
\hline
parallel & fixed-window frequency & 74 \\
parallel & disjoint-window frequency & 55 \\
parallel & weighted-window frequency & 56.3 \\
serial & fixed-window frequency & 1000 \\
serial & disjoint-window frequency & 74 \\
serial & weighted-window frequency & 75.1 \\
\end{tabulary}
\caption{The frequency thresholds used for the results of Figure~\ref{fig:tolstoy-runtime-vs-length}}
\label{table:tolstoy-runtime-vs-length-thresholds}
\end{table}


\subsection{Performance comparison with closed episode miner}

In section~\ref{sec:experiments-correctness} we explained that our implementation computes a subset of the episodes that the closed episode miner mines, if the closed episode miner is configured not to restrict its output to closed episodes. Besides comparing the output of both implementations to ensure the correctness, we can also compare the runtimes.

We ran both implementations on the \emph{tolstoy} dataset with a window width of 15. Thresholds were chosen based on a compromise between amount of output and time consumption. We should remark again that the closed episode miner finds parallel and serial episodes at the same time, as well as general episodes. So some of the configurations of the closed episode miner were equivalent, apart from the frequency threshold.

Table~\ref{table:closepi-comparison} shows the results. For one of the experiments, the closed episode miner ran out of memory (marked with a dash), when it used over 10~GB of memory after a couple of minutes. The other results are not a great look for our implementation. Even though we are at an advantage in the sense that we only mine one class of episodes at a time, the closed episode miner runs significantly faster.

We think that there must be opportunities left for improving the performance of our implementation. Although we tried to do our best, we are not experts at optimizing code for speed. That being said, we think that in most cases, our implementation produces adequate amounts of output in a reasonable time.

% TODO ^ finish this ^

% the choice of data structures, implementing specialized data structures,

% The closed episode miner uses a different method of mining episodes, and perhaps, for this method and the greater complexity of general episodes required a greater level of optimization for the algorithm to be minimally viable.

% The source code of the closed episode miner is undocumented, so it wasn't straightforward to understand or get inspired by it.

% Still, we think that the performance experiments we did on our implementation would largely hold their validity if it were more optimized, where the runtimes would be reduced by a factor.

% TODO ^ include this or something similar, or not? ***FUTURE ME*** no.

\begin{table}
\begin{tabulary}{\textwidth}{L|l|R|R|R}
episode class & frequency measure & frequency threshold & runtime (s) ours & runtime (s) Closepi \\
\hline
parallel & fixed-window fr. & 15 & 296 & --- \\
serial & fixed-window fr. & 150 & 216 & 8.67 \\
parallel & disjoint-window fr. & 7 & 417 & 24.8 \\
serial & disjoint-window fr. & 10 & 417 & 15.0 \\
parallel & weighted-window fr. & 7 & 127 & 9.59 \\
serial & weighted-window fr. & 10 & 300 & 6.65 \\
\end{tabulary}
\caption{Runtimes for mining episodes with our implementation (\emph{ours}) and the closed episode miner (\emph{Closepi}).}
\label{table:closepi-comparison}
\end{table}




\section{Quality}

For the qualitative analysis of the output of our algorithm, we compare the top-ranked results across the different frequency measures, and to those of cohesion-based interestingness measures, which we will clarify in the relevant section.


\subsection{Analysis of episodes mined from \emph{tolstoy} dataset}
\label{sec:experiments-quality-episodes}

As we would expect, if we rank the output by frequency, the top contains mostly just episodes of size 1 (Table~\ref{table:fmw-tolstoy-top-15-parallel-episodes}). It does give us some information about the text, though not much more than when we simply count the occurrences of all words.

\begin{itemize}
\item We learn of many characters' names, as obviously those are mentioned often, but since we see mostly 1-episodes we can't connect many characters' first name and surname.
\item Looking at the column for the disjoint-window frequency, we see that two names are frequently mentioned together: $ \{ \text{alexei}, \text{alexandrovitch} \} $. From this information it is likely that a character named \emph{Alexei Alexandrovitch} appears often in the book. (This is indeed the case.) Moreover, we see that $ \{ \text{alexei}, \text{alexandrovitch} \} $ is just as frequent as subepisode $ \{ \text{alexandrovitch} \} $. So wherever the last name \emph{Alexandrovitch} is mentioned, the first name \emph{Alexei} is mentioned nearby (within at most 15 words).
\item Common words like \emph{thought}, \emph{smile}, \emph{face}, \emph{love}, \emph{eye} (stemmed to \emph{ey}), \emph{feel} can give some indication of genre. At least it seems clear that the sequence does not represent a research paper in computer science.
\end{itemize}

\begin{table}\mytablesize
\begin{tabulary}{\textwidth}{R|L|L|L}%
\# & fixed-window fr. & disjoint-window fr. & weighted-window fr. \\
\hline
1 & $ \{ \text{levin} \} $ (20913) & $ \{ \text{levin} \} $ (1629) & $ \{ \text{levin} \} $ (1629) \\
2 & $ \{ \text{vronski} \} $ (11165) & $ \{ \text{vronski} \} $ (865) & $ \{ \text{vronski} \} $ (865) \\
3 & $ \{ \text{anna} \} $ (10699) & $ \{ \text{anna} \} $ (823) & $ \{ \text{anna} \} $ (823) \\
4 & $ \{ \text{thought} \} $ (8994) & $ \{ \text{kitti} \} $ (672) & $ \{ \text{kitti} \} $ (672) \\
5 & $ \{ \text{time} \} $ (8948) & $ \{ \text{thought} \} $ (663) & $ \{ \text{thought} \} $ (663) \\
6 & $ \{ \text{kitti} \} $ (8826) & $ \{ \text{time} \} $ (651) & $ \{ \text{time} \} $ (651) \\
7 & $ \{ \text{hand} \} $ (8645) & $ \{ \text{hand} \} $ (651) & $ \{ \text{hand} \} $ (651) \\
8 & $ \{ \text{alexei} \} $ (8619) & $ \{ \text{smile} \} $ (632) & $ \{ \text{smile} \} $ (632) \\
9 & $ \{ \text{smile} \} $ (8549) & $ \{ \text{alexei} \} $ (632) & $ \{ \text{alexei} \} $ (632) \\
10 & $ \{ \text{face} \} $ (8315) & $ \{ \text{face} \} $ (598) & $ \{ \text{face} \} $ (598) \\
11 & $ \{ \text{ey} \} $ (8062) & $ \{ \text{love} \} $ (595) & $ \{ \text{love} \} $ (595) \\
12 & $ \{ \text{alexandrovitch} \} $ (7842) & $ \{ \text{alexandrovitch} \} $ (571) & $ \{ \text{alexandrovitch} \} $ (571) \\
13 & $ \{ \text{felt} \} $ (7753) & $ \{ \text{alexei},\allowbreak\text{alexandrovitch} \} $ (571) & $ \{ \text{ey} \} $ (570) \\
14 & $ \{ \text{man} \} $ (7751) & $ \{ \text{ey} \} $ (570) & $ \{ \text{man} \} $ (565) \\
15 & $ \{ \text{feel} \} $ (7596) & $ \{ \text{man} \} $ (565) & $ \{ \text{feel} \} $ (561) \\
\end{tabulary}%
\caption{The top 15 parallel episodes found by our algorithm, with $ \rho = 15 $, and for the three frequency measures.}
\label{table:fmw-tolstoy-top-15-parallel-episodes}
\end{table}

From studying Table~\ref{table:fmw-tolstoy-top-15-parallel-episodes} it is clear that simply ranking episodes by frequency is not a good strategy for getting the most out of our algorithm. We should at least filter out the 1-episodes, as those don't give any more information than counting the support of each word that appears in the text. Table~\ref{table:fmw-tolstoy-top-15-parallel->1-episodes} and Table~\ref{table:fmw-tolstoy-top-15-serial->1-episodes} show the rankings of greater-than-1-episodes, for parallel and serial episodes respectively.

After filtering out 1-episodes we learn some more things:
\begin{itemize}
\item We find full names --- Alexei Alexandrovitch, Stepan Arkadyevitch, Sergei Ivanovitch, Lidia Ivanovna, Darya Alexandrovna, Agafea Mihalovna --- all are characters' full names. With serial episodes, we find their order as well --- \emph{alexei} usually precedes \emph{alexandrovitch} closely, so $ \text{alexei} \to \text{alexandrovitch} $ is rated more highly than $ \text{alexandrovitch} \to \text{alexei} $.
\item We also find the most important couples, since naturally their names are often mentioned close to each other --- Kitty and Levin, Anna and Vronski.
\item The weighted-window frequency finds $ \{ \text{draw}, \text{room} \} $ and $ \text{draw} \to \text{room} $ at positions 10 and 7, respectively. The fixed-window confidence ranked $ \{ \text{draw}, \text{room} \} $ at position 30, and the disjoint-window frequency ranked it at position 90. The weighted-window frequency ranks it highly because of the noun \emph{drawing room}. Those two words often follow each other directly, making for many minimal windows of great weight.
\end{itemize}

\begin{table}\mytablesize
\begin{tabulary}{\textwidth}{R|L|L|L}
\# & fixed-window fr. & disjoint-window fr. & weighted-window fr. \\
\hline
1 & $ \{ \text{alexei},\allowbreak\text{alexandrovitch} \} $ (7416) & $ \{ \text{alexei},\allowbreak\text{alexandrovitch} \} $ (571) & $ \{ \text{alexei},\allowbreak\text{alexandrovitch} \} $ (286) \\
2 & $ \{ \text{stepan},\allowbreak\text{arkadyevitch} \} $ (7117) & $ \{ \text{stepan},\allowbreak\text{arkadyevitch} \} $ (547) & $ \{ \text{stepan},\allowbreak\text{arkadyevitch} \} $ (274) \\
3 & $ \{ \text{sergei},\allowbreak\text{ivanovitch} \} $ (3763) & $ \{ \text{levin},\allowbreak\text{levin} \} $ (395) & $ \{ \text{sergei},\allowbreak\text{ivanovitch} \} $ (146) \\
4 & $ \{ \text{levin},\allowbreak\text{levin} \} $ (3348) & $ \{ \text{sergei},\allowbreak\text{ivanovitch} \} $ (291) & $ \{ \text{darya},\allowbreak\text{alexandrovna} \} $ (102) \\
5 & $ \{ \text{darya},\allowbreak\text{alexandrovna} \} $ (2739) & $ \{ \text{darya},\allowbreak\text{alexandrovna} \} $ (205) & $ \{ \text{levin},\allowbreak\text{levin} \} $ (61.6) \\
6 & $ \{ \text{levin},\allowbreak\text{kitti} \} $ (2039) & $ \{ \text{levin},\allowbreak\text{kitti} \} $ (202) & $ \{ \text{lidia},\allowbreak\text{ivanovna} \} $ (54) \\
7 & $ \{ \text{anna},\allowbreak\text{vronski} \} $ (1942) & $ \{ \text{stepan},\allowbreak\text{levin} \} $ (199) & $ \{ \text{anna},\allowbreak\text{vronski} \} $ (45.1) \\
8 & $ \{ \text{arkadyevitch},\allowbreak\text{levin} \} $ (1896) & $ \{ \text{arkadyevitch},\allowbreak\text{levin} \} $ (197) & $ \{ \text{smile},\allowbreak\text{levin} \} $ (41.3) \\
9 & $ \{ \text{stepan},\allowbreak\text{levin} \} $ (1887) & $ \{ \text{stepan},\allowbreak\text{arkadyevitch},\allowbreak\text{levin} \} $ (195) & $ \{ \text{levin},\allowbreak\text{kitti} \} $ (41.1) \\
10 & $ \{ \text{stepan},\allowbreak\text{arkadyevitch},\allowbreak\text{levin} \} $ (1784) & $ \{ \text{vronski},\allowbreak\text{vronski} \} $ (191) & $ \{ \text{room},\allowbreak\text{draw} \} $ (40.6) \\
11 & $ \{ \text{smile},\allowbreak\text{levin} \} $ (1778) & $ \{ \text{anna},\allowbreak\text{vronski} \} $ (180) & $ \{ \text{countess},\allowbreak\text{lidia} \} $ (39.4) \\
12 & $ \{ \text{vronski},\allowbreak\text{vronski} \} $ (1722) & $ \{ \text{smile},\allowbreak\text{levin} \} $ (171) & $ \{ \text{thought},\allowbreak\text{levin} \} $ (39.3) \\
13 & $ \{ \text{time},\allowbreak\text{levin} \} $ (1597) & $ \{ \text{anna},\allowbreak\text{anna} \} $ (170) & $ \{ \text{love},\allowbreak\text{love} \} $ (38.2) \\
14 & $ \{ \text{brother},\allowbreak\text{levin} \} $ (1558) & $ \{ \text{time},\allowbreak\text{levin} \} $ (159) & $ \{ \text{arkadyevitch},\allowbreak\text{levin} \} $ (37.2) \\
15 & $ \{ \text{anna},\allowbreak\text{anna} \} $ (1531) & $ \{ \text{good},\allowbreak\text{levin} \} $ (153) & $ \{ \text{agafea},\allowbreak\text{mihalovna} \} $ (37) \\
\end{tabulary}%
\caption{The top 15 parallel episodes found by our algorithm, excluding 1-episodes, with $ \rho = 15 $, and for the three frequency measures.}
\label{table:fmw-tolstoy-top-15-parallel->1-episodes}
\end{table}

\begin{table}\mytablesize
\begin{tabulary}{\textwidth}{R|L|L|L}
\# & disjoint-window fr. & weighted-window fr. \\
\hline
1 & $ \text{alexei} \to \text{alexandrovitch} $ (7401) & $ \text{alexei} \to \text{alexandrovitch} $ (571) & $ \text{alexei} \to \text{alexandrovitch} $ (286) \\
2 & $ \text{stepan} \to \text{arkadyevitch} $ (7106) & $ \text{stepan} \to \text{arkadyevitch} $ (547) & $ \text{stepan} \to \text{arkadyevitch} $ (274) \\
3 & $ \text{sergei} \to \text{ivanovitch} $ (3758) & $ \text{levin} \to \text{levin} $ (395) & $ \text{sergei} \to \text{ivanovitch} $ (146) \\
4 & $ \text{levin} \to \text{levin} $ (3348) & $ \text{sergei} \to \text{ivanovitch} $ (291) & $ \text{darya} \to \text{alexandrovna} $ (102) \\
5 & $ \text{darya} \to \text{alexandrovna} $ (2734) & $ \text{darya} \to \text{alexandrovna} $ (205) & $ \text{levin} \to \text{levin} $ (61.6) \\
6 & $ \text{vronski} \to \text{vronski} $ (1722) & $ \text{vronski} \to \text{vronski} $ (191) & $ \text{lidia} \to \text{ivanovna} $ (54) \\
7 & $ \text{anna} \to \text{anna} $ (1531) & $ \text{anna} \to \text{anna} $ (170) & $ \text{draw} \to \text{room} $ (40.5) \\
8 & $ \text{lidia} \to \text{ivanovna} $ (1438) & $ \text{alexandrovitch} \to \text{alexei} $ (145) & $ \text{countess} \to \text{lidia} $ (39.1) \\
9 & $ \text{love} \to \text{love} $ (1411) & $ \text{kitti} \to \text{kitti} $ (144) & $ \text{love} \to \text{love} $ (38.2) \\
10 & $ \text{arkadyevitch} \to \text{levin} $ (1245) & $ \text{love} \to \text{love} $ (140) & $ \text{agafea} \to \text{mihalovna} $ (37) \\
11 & $ \text{kitti} \to \text{kitti} $ (1200) & $ \text{arkadyevitch} \to \text{levin} $ (137) & $ \text{levin} \to \text{felt} $ (32) \\
12 & $ \text{levin} \to \text{kitti} $ (1160) & $ \text{levin} \to \text{kitti} $ (136) & $ \text{vronski} \to \text{vronski} $ (31.3) \\
13 & $ \text{vronski} \to \text{anna} $ (1137) & $ \text{stepan} \to \text{levin} $ (132) & $ \text{good} \to \text{humor} $ (29.1) \\
14 & $ \text{levin} \to \text{felt} $ (1130) & $ \text{stepan} \to \text{arkadyevitch} \to \text{levin} $ (132) & $ \text{anna} \to \text{arkadyevna} $ (28.5) \\
15 & $ \text{draw} \to \text{room} $ (1126) & $ \text{kitti} \to \text{levin} $ (131) & $ \text{vronski} \to \text{anna} $ (28.1) \\
\end{tabulary}
\caption{The top 15 serial episodes found by our algorithm, excluding 1-episodes, with $ \rho = 15 $, and for the three frequency measures.}
\label{table:fmw-tolstoy-top-15-serial->1-episodes}
\end{table}

% TODO elaborate on anti-monotonic frequency measures disadvantageing larger episodes, etc
As we have seen, just showing a limited number of top-ranked results does not suffice to find the most interesting patterns. This is a problem inherent to anti-monotonic frequency measures --- they put larger episodes at a disadvantage, since an episode is never more frequent than any of its subepisodes. $ (l + 1) $-episodes generally score lower than $ l $-episodes, and as a consequence, larger episodes tend to be ranked lower than smaller ones.

Nevertheless, Figure~\ref{fig:episode-frequencies-by-size} shows that, lower thresholds produce greater amounts of large episodes. So if we're specifically interested in those, there are ways to surface them; for example:
\begin{itemize}
\item As we did in this section, we can filter out smaller episodes.
\item Another option is to group episodes by size before ranking them.
\item One could also boost the score of larger episodes in some way, so that they have more value in the rankings.
\end{itemize}



\subsection{Analysis of association rules mined from \emph{tolstoy} dataset}

% The algorithm always uses the associated frequency and confidence measures together. For instance, association rules mined by the disjoint-window frequency will generate association rules according to the minimal-window confidence.

Table~\ref{table:tolstoy-rules-par-fwi} shows the top 15 association rules consisting of parallel episodes using the fixed-window confidence.

For the fixed-window confidence, there are a lot of rules of confidence 1. In fact, in this experiment, there were $ 129 $ association rules consisting of serial episodes that had a confidence of 1, and $ 114\,230 $ such rules for parallel episodes. For minimal windows, the situation wasn't much better: $ 2\,218 $ association rules had a confidence of 1. This poses a problem for top-k techniques. The top-5 lists that we show here are just a small sample of equally-rated association rules.

In the top 5 for the fixed-window confidence, all of the rules have the same tail:
\begin{align*}
\{ \text{stepan},\allowbreak\text{arkadyevitch},\allowbreak\text{ey},\allowbreak\text{love},\allowbreak\text{youth},\allowbreak\text{gallant},\allowbreak\text{steed},\allowbreak\text{token},\allowbreak\text{declaim} \}
\end{align*}
In fact, the output contains 63 of these rules, all with a confidence of 1. This episode originates from a quote that is mentioned twice in the book. A non-stemmed quotation:
\begin{quotation}
``I know a gallant steed by tokens sure,

And by his eyes I know a youth in love,''

declaimed Stepan Arkadyevitch.
\end{quotation}
The heads of these association rules all include the two event types that are outermost in the occurrence, \emph{gallant} and \emph{arkadyevitch}, and leave out different words from the middle of the quote: \emph{stepan}, \emph{ey}, \emph{love}, and so on. All of those subepisosdes are covered by the same fixed windows as the tail, resulting in a fixed-window confidence of 1. Needless to say, including all of these rules is not very informative. Here, restricting the output to closed association rules would be very helpful. The association rule
\begin{align*}
& \{ \text{arkadyevitch},\allowbreak\text{gallant},\allowbreak\text{declaim} \} \Rightarrow \\
& \{ \text{stepan},\allowbreak\text{arkadyevitch},\allowbreak\text{ey},\allowbreak\text{love},\allowbreak\text{youth},\allowbreak\text{gallant},\allowbreak\text{steed},\allowbreak\text{token},\allowbreak\text{declaim} \}
\end{align*}
has a minimal head in order to have a confidence of 1, and the heads of all other 62 rules are superepisodes of this minimal head. So the other 62 rules provide no additional information.

\begin{table}\mytablesize
\begin{tabulary}{\textwidth}{R|L}
\# & association rule (fixed-window confidence) \\
\hline
1 & $ \{ \text{arkadyevitch},\allowbreak\text{ey},\allowbreak\text{love},\allowbreak\text{youth},\allowbreak\text{gallant},\allowbreak\text{steed},\allowbreak\text{token},\allowbreak\text{declaim} \} \Rightarrow \{ \text{stepan},\allowbreak\text{arkadyevitch},\allowbreak\text{ey},\allowbreak\text{love},\allowbreak\text{youth},\allowbreak\text{gallant},\allowbreak\text{steed},\allowbreak\text{token},\allowbreak\text{declaim} \} $ (1) \\
2 & $ \{ \text{stepan},\allowbreak\text{arkadyevitch},\allowbreak\text{love},\allowbreak\text{youth},\allowbreak\text{gallant},\allowbreak\text{steed},\allowbreak\text{token},\allowbreak\text{declaim} \} \Rightarrow \{ \text{stepan},\allowbreak\text{arkadyevitch},\allowbreak\text{ey},\allowbreak\text{love},\allowbreak\text{youth},\allowbreak\text{gallant},\allowbreak\text{steed},\allowbreak\text{token},\allowbreak\text{declaim} \} $ (1) \\
3 & $ \{ \text{arkadyevitch},\allowbreak\text{love},\allowbreak\text{youth},\allowbreak\text{gallant},\allowbreak\text{steed},\allowbreak\text{token},\allowbreak\text{declaim} \} \Rightarrow \{ \text{stepan},\allowbreak\text{arkadyevitch},\allowbreak\text{ey},\allowbreak\text{love},\allowbreak\text{youth},\allowbreak\text{gallant},\allowbreak\text{steed},\allowbreak\text{token},\allowbreak\text{declaim} \} $ (1) \\
4 & $ \{ \text{arkadyevitch},\allowbreak\text{ey},\allowbreak\text{youth},\allowbreak\text{gallant},\allowbreak\text{steed},\allowbreak\text{token},\allowbreak\text{declaim} \} \Rightarrow \{ \text{stepan},\allowbreak\text{arkadyevitch},\allowbreak\text{ey},\allowbreak\text{love},\allowbreak\text{youth},\allowbreak\text{gallant},\allowbreak\text{steed},\allowbreak\text{token},\allowbreak\text{declaim} \} $ (1) \\
5 & $ \{ \text{stepan},\allowbreak\text{arkadyevitch},\allowbreak\text{youth},\allowbreak\text{gallant},\allowbreak\text{steed},\allowbreak\text{token},\allowbreak\text{declaim} \} \Rightarrow \{ \text{stepan},\allowbreak\text{arkadyevitch},\allowbreak\text{ey},\allowbreak\text{love},\allowbreak\text{youth},\allowbreak\text{gallant},\allowbreak\text{steed},\allowbreak\text{token},\allowbreak\text{declaim} \} $ (1) \\
\end{tabulary}
\caption{Top 5 parallel association rules, by the fixed-window confidence.}
\label{table:tolstoy-rules-par-fwi}
\end{table}

Table~\ref{table:tolstoy-rules-par-wwi} shows the top 5 association rules consisting of parallel episodes using the weighted-window confidence, and Table~\ref{table:tolstoy-rules-ser-wwi} shows the same for serial episodes.

From this top 5 we mostly learn that for certain characters, the full names are always used.

\begin{itemize}
\item Parallel \#1: Whenever two occurrences of \emph{arkadyevitch} are close together, \emph{stepan} can be found nearby. This is because the character \emph{Stepan Arkadyevitch} is never addressed by his first name only.
\item Parallel and serial \#2: The same for \emph{Alexei Alexandrovitch}.
\item Parallel \#3 and \#5 give more or less the same information as \#1.
\item Serial \#1: $ \text{sergei} \to \text{levin} $ leads consistenly to $ \text{sergei} \to \text{ivanovitch} \to \text{levin} $. Sergei Ivanovitch Levin is one character's full name, so it makes sense that whenever we find the outermost parts, the inner part is in between.
\item ...
\end{itemize}

% TODO finish

For the weighted-window confidence, there were far fewer association rules in the top 15 with the same confidence value.

However, we should note that the comparison is not entirely fair --- the sets of episodes upon which we built the association rules depended both on the computational constraints of the algorithms implementing the respective frequency measures, and on the biases that each frequency measure. For instance, during the performance experiments it became clear that the fixed-window frequency was able to find more episodes in the same amount of time. Also, the weighted-window frequency 

If we wanted to make a better qualitative comparison between the different confidence measures, perhaps we should decide on a set of episodes, and generate confidence rules based on those. While not hard to realize, this would require some time to modify the implementation, and could be subject of future work.

Because of the aforementioned, it is hard to make definitive statements about how the confidence measures compare in quality. We did, however, see that some elimination of redundancy would benefit all three confidence measures.

\begin{table}\mytablesize
\begin{tabulary}{\textwidth}{R|L}

\# & association rule (weighted-window confidence) \\
\hline
1 & $ \{ \text{arkadyevitch}, \text{arkadyevitch} \} \Rightarrow \{ \text{stepan}, \text{arkadyevitch}, \text{arkadyevitch} \} $ (1) \\
2 & $ \{ \text{alexandrovitch}, \text{alexandrovitch} \} \Rightarrow \{ \text{alexei}, \text{alexandrovitch}, \text{alexandrovitch} \} $ (1) \\
3 & $ \{ \text{stepan}, \text{stepan} \} \Rightarrow \{ \text{stepan}, \text{stepan}, \text{arkadyevitch} \} $ (1) \\
4 & $ \{ \text{countess}, \text{ivanovna} \} \Rightarrow \{ \text{countess}, \text{lidia}, \text{ivanovna} \} $ (0.943) \\
5 & $ \{ \text{arkadyevitch}, \text{alexei} \} \Rightarrow \{ \text{stepan}, \text{arkadyevitch}, \text{alexei} \} $ (0.937) \\

\end{tabulary}
\caption{Top 5 parallel association rules, by the weighted-window confidence.}
\label{table:tolstoy-rules-par-wwi}
\end{table}
\begin{table}\mytablesize
\begin{tabulary}{\textwidth}{R|L}
\# & association rule (weighted-window confidence) \\
\hline
1 & $ \text{sergei} \to \text{levin} \Rightarrow \text{sergei} \to \text{ivanovitch} \to \text{levin} $ (1) \\
2 & $ \text{alexandrovitch} \to \text{alexandrovitch} \Rightarrow \text{alexandrovitch} \to \text{alexei} \to \text{alexandrovitch} $ (1) \\
3 & $ \text{stepan} \to \text{levin} \Rightarrow \text{stepan} \to \text{arkadyevitch} \to \text{levin} $ (1) \\
4 & $ \text{stepan} \to \text{smile} \Rightarrow \text{stepan} \to \text{arkadyevitch} \to \text{smile} $ (1) \\
5 & $ \text{alexei} \to \text{alexandrovitch} \to \text{alexandrovitch} \Rightarrow \text{alexei} \to \text{alexandrovitch} \to \text{alexei} \to \text{alexandrovitch} $ (1) \\
\end{tabulary}
\caption{Top 5 serial association rules, by the weighted-window confidence.}
\label{table:tolstoy-rules-ser-wwi}
\end{table}

\subsection{Comparison with non-frequency-based methods}

\iffalse
As we saw in Section~\ref{sec:experiments-quality-episodes}, anti-monotonic frequency measures inherently put larger episodes at a disadvantage, since an episode is never more frequent than any of its subepisodes.
\fi

So far, in this thesis, we have only studied frequency-based interestingness measures. In this section, we'll look into a different interpretation of interestingness: \emph{cohesion}. The cohesiveness of an episode expresses how closely the events of its occurrences are to each other. An episode is deemed cohesive in regard to a sequence if the events that constitute occurrences are close to each other; if, in other words, its minimal windows are generally small.

All frequency measures we have studied so far had a cohesion angle as well. This is natural in the context of a long event sequence, where events that are far apart, are unlikely to have anything to do with each other. The width of the sliding window was not only a means to make the algorithms computationally feasible, it also placed a cutoff beyond which events were deemed uncorrelated. For the disjoint-window frequency, this cutoff was quite harsh --- all minimal windows within a certain width $ \rho $ were considered equally valuable, while any windows larger than $ \rho $ were judged uncorrelated.

The weighted-window frequency takes into account the width of the minimal windows more carefully --- it assigns a weight to each minimal window (the inverse of its width) and sums the weights. So, the weighted-window frequency places importance on the cohesiveness of patterns, but it remains a measure based on frequency.

With cohesion-based interestingness measures, the main focus shifts towards the cohesion of patterns.

In~\cite{cule2016efficient}, the cohesion of an episode $ \alpha $ is defined as
\begin{align*}
C(\alpha) = \frac{| \alpha |}{\overline{W}(\alpha)}
\end{align*}
where $ \overline{W}(\alpha) $ is the average width of the minimal windows of $ \alpha $. Further details on the exact definition can be found in~\cite{cule2016efficient}.

This definition makes an effort not to disadvantage larger patterns --- as anti-monotonic frequency measures inherently do for computational reasons --- by defining the cohesiveness of an episode to be proportional in the size of the episode. The paper describes a method for mining episodes according to this non-monotonic measure, using other techniques of pruning the search space, which are beyond the scope of this thesis.

Also, to prevent including patterns that are too infrequent to bear any kind of significance, \cite{cule2016efficient} places a frequency constraint as well: the event types that make up a cohesive episode should occur often enough by themselves, given some support threshold. Event types that don't have enough support won't appear in any pattern.

The mining algorithm from~\cite{cule2016efficient}, called \textsc{Fci}, mines parallel episodes with an injective $ lab $-function, meaning that each event type appears at most once. The algorithm takes the following parameters:
\begin{itemize}
\item The minimal support that an event type must have in order to be considered at all.
\item The maximal size of patterns to generate.
\item The minimal cohesion that any pattern must have.
\end{itemize}

In~\cite{cule2016efficient}, the cohesion of a pattern is defined using the mean width of the minimal windows. The mean has a number of downsides, though; one of which is that the mean of a distribution is unstable if there are outliers. So if one window is significantly larger or smaller than the others, the cohesion may be greatly affected. \citep{feremans2018mining} proposes a quantile-based approach. Here, the minimal occurrences are divided into two groups based on a width threshold. The quantile-based cohesion is then defined as the percentage of minimal occurrences that are smaller than the threshold. The threshold is proportional to the size of the episode, again in an effort not to disadvantage larger patterns.

Their mining method \textsc{Qcsp} mines serial episodes, and takes the following parameters:
\begin{itemize}
\item The minimal support that an event type must have in order to be considered at all.
\item The maximal size of patterns to generate.
\item A minimal-window width threshold (as mentioned above, which gets multiplied by the size of each episode being considered).
\item $ k $. The algorithm reports only the $ k $ patterns with the greatest cohesion.
\end{itemize}

Again we refer to the paper for more details \cite{feremans2018mining}.

We ran \textsc{Fci} with a minimum support of 5, a maximal pattern size of 5, and a minimal cohesion of 0.014 (a lower threshold started to take significantly more time); and \textsc{Qcsp} with a minimum support of 5, a maximal pattern size of 7 and a window-width threshold of 2.

% TODO check window-width threshold

\begin{table}\mytablesize
\centering
\begin{tabulary}{\textwidth}{R|L|L}

\# & mean-based cohesion & quantile-based cohesion (cohesion, support) \\
\hline
1 & $ \{ \text{agafea}, \text{mihalovna} \} $ (1) & $ \text{stepan} \to \text{arkadyevitch} $ (0.998, 1096) \\
2 & $ \{ \text{char}, \text{banc} \} $ (1) & $ \text{alexei} \to \text{alexandrovitch} $ (0.95, 1203) \\
3 & $ \{ \text{pinc}, \text{nez} \} $ (1) & $ \text{sergei} \to \text{ivanovitch} $ (0.965, 603) \\
4 & $ \{ \text{bell}, \text{soeur} \} $ (1) & $ \text{agafea} \to \text{mihalovna} $ (1, 148) \\
5 & $ \{ \text{stepan}, \text{arkadyevitch} \} $ (0.915) & $ \text{darya} \to \text{alexandrovna} $ (0.967, 424) \\
6 & $ \{ \text{nativ}, \text{tribe} \} $ (0.523) & $ \text{char} \to \text{banc} $ (1, 14) \\
7 & $ \{ \text{lizaveta}, \text{petrovna} \} $ (0.408) & $ \text{pinc} \to \text{nez} $ (1, 12) \\
8 & $ \{ \text{ivanovna}, \text{lidia} \} $ (0.137) & $ \text{bell} \to \text{soeur} $ (1, 10) \\
9 & $ \{ \text{alexandrovitch}, \text{alexei} \} $ (0.05) & $ \text{lidia} \to \text{ivanovna} $ (0.977, 221) \\
10 & $ \{ \text{sergei}, \text{ivanovitch} \} $ (0.0428) & $ \text{lizaveta} \to \text{petrovna} $ (0.98, 49) \\
11 & $ \{ \text{darya}, \text{alexandrovna} \} $ (0.0363) & $ \text{nativ} \to \text{tribe} $ (0.966, 29) \\
12 & $ \{ \text{bezzubov}, \text{landau} \} $ (0.0351) & $ \text{mashkin} \to \text{upland} $ (0.833, 12) \\
13 & $ \{ \text{gladiat}, \text{frou} \} $ (0.0326) & $ \text{grand} \to \text{duchess} $ (0.714, 14) \\
14 & $ \{ \text{partnership}, \text{ryezunov} \} $ (0.0303) & $ \text{liza} \to \text{merkalova} $ (0.706, 34) \\
15 & $ \{ \text{bridal}, \text{lectern} \} $ (0.0281) & $ \text{marya} \to \text{nikolaevna} $ (0.673, 98) \\
\end{tabulary}
\caption{The top 15 patterns mined from~\emph{tolstoy} using cohesion (\textsc{Fci}, minimum support 5, maximal size 5) and quantile-based cohesion (\textsc{Qcsp}, minimum support 5, maximal size 5, minimal-window width threshold 2).}
\label{table:cohesive-patterns}
\end{table}

Table~\ref{table:cohesive-patterns} shows the 15 most interesting episodes by the two different definitions of cohesion. The implementation of the quantile-based cohesion sorts episodes according to a linear combination of the cohesion and the support, which is defined as the sum of supports of the constituent event types. Since \textsc{Qcsp} doesn't rank purely on cohesion.

Some of these patterns are both frequent and cohesive --- we see some of the characters' names reappear. But we also see many patterns that we haven't seen before.

The algorithms find some maximally cohesive patterns consisting of multiple-word nouns, for which the constituent words don't appear on their own anywhere else, such as \emph{char--banc} (7 occurrences), \emph{pince-nez} (6 occurrences), and \emph{belle-soeur} (5 occurrences).

These are all quite infrequent. For the disjoint-window frequency, $ \{ \text{char},\allowbreak \text{banc} \} $ appears at position $ 30\,203 $ in the rankings --- which is far down, and rightly so, for it is a frequency measure.

For the weighted-window frequency, $ \{ \text{nativ}, \text{tribe} \} $ appears at position $ 3658 $.

Interestingly, there are no 3-episodes in the top 15, despite the efforts not to disadvantage larger episodes. However, the first 3-episode appears at position 17, and the first 4-episode at position 21. Further down the rankings, many episodes are composed of mostly the same few event types, such as \emph{eye}, \emph{face}, \emph{smile}, \emph{time}, \emph{hand}, \emph{thought}.

The mean-based cohesion starts decreasing very quickly. $ \{ \text{nativ}, \text{tribe} \} $ has a mean-based cohesion of 0.523. The phrase \emph{Native Tribes} occurs 14 times in the book, and the word \emph{tribe} occurs only once on its own. If that occurrence wasn't there, the cohesion of $ \{ \text{nativ}, \text{tribe} \} $ would equal 1. So the mean-based cohesion punishes any unrelated occurrences quite harshly; likely because of the instability to outliers that the mean suffers. Intuitively, given that one lone occurrence, we would still consider the episode to be quite cohesive. While $ \{ \text{nativ}, \text{tribe} \} $ is ranked lower for the quantile-based cohesion, its cohesion value is closer to what we would intuitively expect. We see that cohesion values decrease far less quickly.

Many of these are patterns that we wouldn't have found at the top of the output of any frequency-based measures, for they are quite infrequent. Nevertheless, they are valid patterns, since we can recognize nouns and phrases.

While \textsc{Qcsp} found few greater-than-2-episodes, in our opinion the 2-episodes it did find were more interesting overall. In Table~\ref{table:quantile-cohesion-selection} we give a selection of patterns from the top 150. For the mean-based cohesion we chose ten consecutive patterns which are quite representative of the top 150 beyond the first 30 patterns. The quantile-based cohesion contained much more varied results the top 150. So, at least for this dataset, we think the quantile-based cohesion measure is the better option.

\begin{table}\mytablesize
\centering
\begin{tabulary}{\textwidth}{R|L}
45 & $ \{ \text{chapter},\allowbreak \text{felt},\allowbreak \text{feel},\allowbreak \text{time},\allowbreak \text{thought} \} $ (0.0161) \\
46 & $ \{ \text{chapter},\allowbreak \text{face},\allowbreak \text{smile},\allowbreak \text{time},\allowbreak \text{hand} \} $ (0.016) \\
47 & $ \{ \text{troth},\allowbreak \text{bridal} \} $ (0.016) \\
48 & $ \{ \text{ey},\allowbreak \text{smile},\allowbreak \text{hand} \} $ (0.016) \\
49 & $ \{ \text{face},\allowbreak \text{smile},\allowbreak \text{hand} \} $ (0.0159) \\
50 & $ \{ \text{chapter},\allowbreak \text{ey},\allowbreak \text{face},\allowbreak \text{time},\allowbreak \text{hand} \} $ (0.0159) \\
51 & $ \{ \text{feel},\allowbreak \text{time} \} $ (0.0159) \\
52 & $ \{ \text{look},\allowbreak \text{feel},\allowbreak \text{ey},\allowbreak \text{face},\allowbreak \text{hand} \} $ (0.0159) \\
53 & $ \{ \text{ey},\allowbreak \text{smile},\allowbreak \text{time},\allowbreak \text{hand} \} $ (0.0159) \\
54 & $ \{ \text{ey},\allowbreak \text{face},\allowbreak \text{time},\allowbreak \text{hand} \} $ (0.0159) \\
\end{tabulary}
\caption{Ten consecutive patterns from the top-ranked patterns by \textsc{Fci} (mean-based cohesion).}
\label{table:mean-cohesion-selection}
\end{table}

\begin{table}\mytablesize
\centering
\begin{tabulary}{\textwidth}{R|L}
\# & quantile-based cohesion (cohesion, support) \\
\hline
26 & $ \text{bridal} \to \text{pair} $ (0.5, 20) \\
39 & $ \text{wag} \to \text{tail} $ (0.4, 20) \\
43 & $ \text{roast} \to \text{beef} $ (0.4, 10) \\
52 & $ \text{lightn} \to \text{thunder} $ (0.364, 11) \\
77 & $ \text{unbutton} \to \text{waistcoat} $ (0.3, 20) \\
83 & $ \text{butter} \to \text{salt} $ (0.294, 17) \\
89 & $ \text{summer} \to \text{villa} $ (0.281, 64) \\
107 & $ \text{quicken} \to \text{pace} $ (0.267, 45) \\
112 & $ \text{beaten} \to \text{track} $ (0.267, 15) \\
123 & $ \text{railwai} \to \text{station} $ (0.253, 79) \\
\end{tabulary}
\caption{Ten patterns selected from the top 150 patterns by \textsc{Qcsp} (quantile-based cohesion).}
\label{table:quantile-cohesion-selection}
\end{table}

At the beginning of this section, we saw that all of the frequency-based measures we discussed had some notion of cohesion. And the cohesion-based measures had some notion of frequency as well. So it seems that, whichever interestingness criterion a measure emphasizes, a bit of both is useful.

In conclusion, we can say that the cohesion-based frequency measures certainly have their merit. Using the algorithms we were able to find some interesting patterns that weren't ranked anywhere near the top of the frequency-based rankings.

That isn't to say that either one is superior to the other --- each has their strengths.

[TODO make more coherent, conclude]
