\chapter{Experiments}
\label{sec:experiments}

A big point of comparison while implementing our algorithms was a closed episode miner\footnote{http://adrem.ua.ac.be/mining-closed-strict-episodes}. It allowed us to verify the correctness of our implementation. The closed episode miner differs in a few ways from our implementation:

\begin{itemize}
\item It generates general episodes, not just parallel and serial episodes.
\item It generates closed episodes. Closed episodes are episodes that have no superepisodes of the same frequency. Mining only closed episodes helps in reducing the amount of output. The implementation has options to mine non-closed episodes as well, though, which allows us to compare with our implementation.
\item While our implementation finds episodes of one class at a time, as specified by the user---parallel or serial---the closed episode miner finds episodes of all classes at once---parallel, serial and general.
\end{itemize}

Given the above, in order to compare the two implementations in terms of their output, we have to enable the options that cause the closed episode miner to find non-closed episodes, since our implementation finds non-closed episodes as well; and we have to filter out those episodes which don't match the class of episodes we're currently mining.

Mining general episodes is of higher complexity than parallel and serial episodes.

Parallel episodes can only ``grow'' by adding a new node, and serial episodes grow in much the same way: by adding a node and an edge at the same time. Pattern explosion is a bigger concern with general episodes: additionally, they can grow by adding only an edge. So, ideally, our implementation should be faster than the closed episode miner.

% ^ terrible ^

We'll do a comparative study with two implementations of mining algorithms that use different, non-frequency interestingness measures.

The first interestingness measure is called \emph{cohesion}, introduced in~\citep{cule2016efficient}. The cohesiveness of an episode/itemset expresses how closely the events of its occurrences are to each other, that is, the distance between events is taken into account.

We won't go into detail on the mining algorithm, but we'll take a look at the interestingness measure to see how it compares to the measures we considered.

Episodes are restricted to parallel episodes in which $ lab $ is injective, that is, each event type appears at most once. For an episode $ \alpha = \{ A_1, \ldots, A_n \} $, the set of occurrences of event types making up $ \alpha $ in a sequence $ \boldsymbol{s} $ is defined as
\begin{align*}
N(\alpha) = \{ t \mid (A, t) \in \boldsymbol{s} \wedge \exists v \in V(\alpha): lab(v) = A \}
\end{align*}

The support of an event type $ A $ is $ | N(\{ A \}) | $.



A quantile-based approach in~\citep{feremans2018mining}.

Throughout the experiments, we will consider a few different datasets.

\begin{itemize}
\item \emph{abstract}: a dataset consisting of the first 739 NSF award abstracts from 1990, merged into one long sequence.
\item \emph{trains}: a dataset consisting of departure times of delayed trains in a Belgian railway station, for trains with a delay of at least three minutes.
\end{itemize}

\section{Efficiency}

Nothing... yet. Or is there...? now look at that! a plot! amazing... only took me three months

\begin{tikzpicture}

\begin{axis}[
    title=Run time,
    xlabel={event types ordered by frequency},
    ylabel={time (s)},
    ymode=log
]

\addplot table [x=rank,y=count,mark=none] {experiments/trains-alphabet-frequency.tsv};

\end{axis}

\end{tikzpicture}

\begin{tikzpicture}

\begin{axis}[
    title=Run time,
    xlabel={event types ordered by frequency},
    ylabel={time (s)},
    ymode=log
]

\addplot table [x=rank,y=count,mark=none] {experiments/nsf-alphabet-frequency.tsv};

\end{axis}

\end{tikzpicture}

\begin{tikzpicture}

\begin{axis}

\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-parallel-minimal-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-parallel-weighted-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-serial-minimal-windows-8.tsv};
\addplot table [x=num-frequent-episodes,y=duration-s] {experiments/nsf/nsf-serial-weighted-windows-8.tsv};

\end{axis}

\end{tikzpicture}

\section{Quality}

We consider the example sequence of the example in figure~\ref{fig:event-sequence}, which was used as an example throughout chapter~\ref{sec:problem-statement}. A small sequence is interesting to analyze because we have a full overview of the dataset, and can therefore provide insight into how the frequency and confidence values came to be. Also, it is possible to generate all episodes that cover the sequence for a certain window size, using a low frequency thresold.

We'll generate all episodes using all of the frequency measures we implemented.

\begin{table}

\begin{tabulary}{\textwidth}{ L|R|R|R }



\end{tabulary}

\end{table}
