\chapter{Conclusion}

% TODO something here? something about data science in general again?

After an introduction to sequential pattern mining, and after defining various measures of interestingness of patterns, ranging from long-established approaches to more novel ones, we devised and implemented an algorithm for mining frequent episodes and confident association rules. While the general strategy for mining episodes is consistent across all episode classes and frequency measures --- breadth-first approach, Apriori-style pruning --- quite a few specialized subalgorithms were needed. Many of these subalgorithms were based on preexisting algorithms, though most needed some adaptation for our purposes.

% TODO ^ expand introduction to conclusion ^

We were certainly able to find interesting patterns using these measures.

We saw that each of the interestingness measures had advantages and disadvantages. The implementation of the fixed-window frequency often produced larger amounts of parallel episodes for a given amount of time, since it requires less bookkeeping in a database pass than the other approaches. Furthermore, computing the fixed-window confidence of an association rule is trivial, given the frequency of the episodes from which the rule is composed. While it is suitable for ranking purposes, the fixed-window frequency values are counterintuitive, and depend highly on the chosen window width. Therefore the values themselves should always be interpreted within the context of the chosen window width.

The minimal-windows-based measures produced more natural frequency values. The disjoint-window frequency values minimal windows of all sizes equally, at the risk of finding meaningless patterns if the window width is too large. The weighted-window frequency values smaller minimal windows more highly, which makes it less likely to find spurious patterns, but causes it to be less favourable towards potentially meaningful larger episodes.

While our implementation is most often able to find adequate output in reasonable amounts of time, the closed episode miner often completed a task in significantly less time, despite the fact that we have a computational advantage in that the search space is more restricted.

For the frequency-based measures, it was clear that simply ranking the episodes by frequency is not sufficient to discover all interesting patterns. We need techniques surface larger episodes, for instance by excluding 1-episodes --- they dominate the top of the rankings, yet their support is trivial to compute. One option, as we stated, is to group episodes by size, and rank each of the groups. Another possibility is to account for the size of the episode in the definition of the measure itself, as with the cohesion-based measures.

Possible avenues for future work include further optimizing the performance of the implementation. Furthermore, it could be useful to conduct analysis with more varied datasets, to get an even better picture of the capabilities and the limitations of the measures and of the algorithm. Finally, finding more techniques of increasing the value of the output may be rewarding, such as preventing larger episodes from being inherently at a disadvantage, or exploring options to reduce redundancy, particularly for association rules.
