\chapter{Conclusion}

% TODO something here? something about data science in general again?

After an introduction to sequential pattern mining, and after defining various measures of interestingness of patterns, ranging from long-established approaches to more novel ones, we devised and implemented an algorithm for mining frequent episodes and confident association rules. While the general strategy for mining episodes is consistent across all episode classes and frequency measures --- breadth-first approach, Apriori-style pruning --- quite a few specialized subalgorithms were needed. Many of these subalgorithms were based on preexisting algorithms, though most needed some adaptation for our purposes.

% TODO ^ expand introduction to conclusion ^

We were certainly able to find interesting patterns using these measures.

We saw that each of the interestingness measures had advantages and disadvantages. The implementation of the fixed-window frequency often produced larger amounts of parallel episodes for a given amount of time, since it requires less bookkeeping in a database pass than the other approaches. Furthermore, computing the fixed-window confidence of an association rule is trivial, given the frequency of the episodes from which the rule is composed.

The minimal-windows-based measures produced more natural frequency values. Where the disjoint-window frequency takes a step back from the fixed-window frequency in the sense that smaller minimal occurrences should be valued more highly, the weighted-window frequency is successful in solving that flaw, while keeping the advantages of the minimal-windows-based approach.

% TODO ^ more advantages, disadvantages ^

While our implementation is most often able to find adequate output in reasonable amounts of time, the closed episode miner often completed a task in significantly less time, despite the fact that we have a computational advantage in that the search space is more restricted.

For the frequency-based measures, it was clear that we need techniques not to disadvantage larger episodes as much, such as excluding 1-episodes --- they dominate the top of the rankings, yet are trivial to compute. One option, as we stated, is to group episodes by size, and rank each of the groups. Another possibility is to account for the size of the episode in the definition of the measure itself, as is the case for cohesion.

Possible avenues for future work include further optimizing the performance of the implementation. Though we tried our best to do so --- see the \emph{notes on the implementation} sections --- we aren't experts at optimizing code, so there should be opportunities. Furthermore, it could be useful to conduct analysis with more varied datasets, to get an even better picture of the capabilities and the limitations of the measures and of the algorithm. Finally, finding more techniques of increasing the value of the output may be rewarding, such as preventing larger episodes from being inherently at a disadvantage, or exploring options to reduce redundancy, particularly for association rules.
