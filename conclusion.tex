\chapter{Conclusion}

After an introduction to sequential pattern mining, and after defining various measures of interestingness of patterns, ranging from long-established approaches to more novel ones, we devised and implemented an algorithm for mining frequent episodes and confident association rules. While the general strategy for mining episodes was consistent across all episode classes and frequency measures --- breadth-first approach, Apriori-style pruning --- quite a few specialized subalgorithms were needed. Many of these subalgorithms were based on preexisting algorithms, though most needed some adaptation for our purposes.

We saw that each of the interestingness measures have advantages and disadvantages. The implementation of the fixed-window frequency for parallel episodes performed the best, since it requires less bookkeeping in a database pass than the other approaches. Furthermore, computing the fixed-window confidence of an association rule is trivial, where the minimal-windows-based confidence measures require some more work. The fixed-window frequency values are counterintuitive, and depend highly on the chosen window width. Therefore the values themselves should always be interpreted within the context of the chosen window width.

The minimal-windows-based measures produced more natural frequency values. The disjoint-window frequency values minimal windows of all sizes equally, at the risk of finding meaningless patterns if the window width is too large. The weighted-window frequency values smaller minimal windows more highly, which makes it less likely to find spurious patterns, but causes it to be less favourable towards potentially meaningful, larger episodes.

For all frequency-based measures, it was clear that simply ranking the episodes by frequency is not sufficient to discover all interesting patterns. We need techniques surface larger episodes, for instance by excluding smaller episodes --- 1-episodes dominate the top of the rankings, yet their support is trivial to compute.
% One option, as we stated, is to group episodes by size, and rank each of the groups. Another possibility is to account for the size of the episode in the definition of the measure itself, as with the cohesion-based measures.

Nevertheless, we were able to find some interesting patterns in Tolstoy's \emph{Anna Karenina} by constructing the top 15 episodes after filtering out the 1-episodes.

The association rules were in need of redundancy elimination. In particular, the rankings of the fixed-window confidence contained way too many association rules of confidence 1, with many giving no additional information beyond a more general rule also present.

The experiments with the cohesion-based measures turned out to be a valuable complement to those for the frequency-based measures. They found some interesting cohesive patterns that weren't very frequent --- episodes which a frequency-based measure would not score highly, but which, due to their minimal windows being small, were clearly related semantically.

Possible avenues for future work include further optimizing the performance of the implementation --- while our implementation is most often able to find adequate output in reasonable amounts of time, the closed episode miner often completed a task in significantly less time, despite the fact that we have a computational advantage in that the search space is more restricted. Furthermore, it could be useful to conduct analysis with more varied datasets, to get an even better picture of the capabilities and the limitations of the measures and of the algorithm. Finally, finding more techniques of increasing the value of the output may be rewarding, such as preventing larger episodes from being inherently at a disadvantage, or exploring options to reduce redundancy, particularly for association rules.
