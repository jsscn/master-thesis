\chapter{Conclusion}

% TODO something here? something about data science in general again?

After an introduction to sequential pattern mining, including various measures of interestingness of patterns, ranging from long-established approaches to more novel ones, we implemented an algorithm for mining frequent episodes and confident association rules, according to those interestingness measures. While the general strategy for mining episodes is consistent across these episode classes and frequency measures---breadth-first search, Apriori-style pruning---quite a few specialized subalgorithms were needed. Many of the subalgorithms were based on preexisting algorithms, though most needed some adaptation for our purposes.

% TODO ^ expand introduction to conclusion ^

We saw that each of these interestingness measures had advantages and disadvantages. The implementation of the fixed-window frequency often produced larger amounts of parallel episodes in less time, since the amount of bookkeeping in a database pass is smaller compared to the other approaches. Furthermore, computing the fixed-window confidence of an association rule is trivial, given the frequency of the episodes from which the rule is composed. However, 

The measures based on minimal windows performed slower, but they produce more natural frequency values.

% TODO ^ more advantages, disadvantages ^

There are downsides to our implementation.
- efficiency often worse than closed episode miner
- no closed episodes
- no general episodes
- no association rules from both parallel and serial episodes at once
- output quality suffers due to redundancy (closedness plays into this as well)

recap algorithms; different algorithms for both episode classes

experiments

efficiency: episodes (classes, frequency measures, window widths), association rules

output quality: better with closed and stuff (need redundancy-eliminating techniques)

need techniques not to disadvantage larger episodes as much

We saw that the computationally less complex interestingness measures often performed better in terms of output size, which can be advantageous when trying to lower the frequency threshold as much as possible. However, more complex measures certainly have advantages of their own: they often make more careful considerations when deciding the fate of a poor episode.

with fixed window frequency easy to generate very many parallel episodes and association rules.

fixed-window frequency/confidence have theoretical disadvantages but practical advantages
other measures more theoretically advantageous but less practically feasible
